{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** In this project we are using the ResNet architecture for our CNN encoder, for the RNN i used LSTM, for the embed_size and hidden_size i used 512 this value was taken from [this paper](https://arxiv.org/pdf/1502.03044.pdf), for the batch size i used 32, as mentioned in the minibatch size lesson using values between 1-32 is a good bet\n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** The transform_train is good for our architecture, it resizes to 256 and crops the image to 224,224 which is what our model needs as input\n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** The ResNet is a pre-trained architecture, so we don't need to retrain it, but we need to train the RNN decoder\n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** I choose Adam because paper mentions that Adam was used instead of other algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/414113 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 610/414113 [00:00<01:07, 6096.98it/s]\u001b[A\n",
      "  0%|          | 1139/414113 [00:00<01:10, 5829.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.82s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "  0%|          | 1741/414113 [00:00<01:10, 5885.33it/s]\u001b[A\n",
      "  1%|          | 2338/414113 [00:00<01:09, 5907.79it/s]\u001b[A\n",
      "  1%|          | 2932/414113 [00:00<01:09, 5917.16it/s]\u001b[A\n",
      "  1%|          | 3526/414113 [00:00<01:09, 5922.05it/s]\u001b[A\n",
      "  1%|          | 4126/414113 [00:00<01:08, 5944.30it/s]\u001b[A\n",
      "  1%|          | 4725/414113 [00:00<01:08, 5954.24it/s]\u001b[A\n",
      "  1%|▏         | 5324/414113 [00:00<01:08, 5961.07it/s]\u001b[A\n",
      "  1%|▏         | 5920/414113 [00:01<01:08, 5958.93it/s]\u001b[A\n",
      "  2%|▏         | 6523/414113 [00:01<01:08, 5979.94it/s]\u001b[A\n",
      "  2%|▏         | 7119/414113 [00:01<01:08, 5973.03it/s]\u001b[A\n",
      "  2%|▏         | 7711/414113 [00:01<01:08, 5954.52it/s]\u001b[A\n",
      "  2%|▏         | 8319/414113 [00:01<01:07, 5990.08it/s]\u001b[A\n",
      "  2%|▏         | 8925/414113 [00:01<01:07, 6008.47it/s]\u001b[A\n",
      "  2%|▏         | 9524/414113 [00:01<01:07, 5980.01it/s]\u001b[A\n",
      "  2%|▏         | 10121/414113 [00:01<01:08, 5924.24it/s]\u001b[A\n",
      "  3%|▎         | 10713/414113 [00:01<01:08, 5916.99it/s]\u001b[A\n",
      "  3%|▎         | 11304/414113 [00:01<01:08, 5913.68it/s]\u001b[A\n",
      "  3%|▎         | 11904/414113 [00:02<01:07, 5937.42it/s]\u001b[A\n",
      "  3%|▎         | 12502/414113 [00:02<01:07, 5947.24it/s]\u001b[A\n",
      "  3%|▎         | 13107/414113 [00:02<01:07, 5975.52it/s]\u001b[A\n",
      "  3%|▎         | 13705/414113 [00:02<01:07, 5961.83it/s]\u001b[A\n",
      "  3%|▎         | 14314/414113 [00:02<01:06, 5997.66it/s]\u001b[A\n",
      "  4%|▎         | 14914/414113 [00:02<01:06, 5996.49it/s]\u001b[A\n",
      "  4%|▎         | 15514/414113 [00:02<01:06, 5952.99it/s]\u001b[A\n",
      "  4%|▍         | 16113/414113 [00:02<01:06, 5962.57it/s]\u001b[A\n",
      "  4%|▍         | 16716/414113 [00:02<01:06, 5982.51it/s]\u001b[A\n",
      "  4%|▍         | 17315/414113 [00:02<01:06, 5964.50it/s]\u001b[A\n",
      "  4%|▍         | 17926/414113 [00:03<01:05, 6006.94it/s]\u001b[A\n",
      "  4%|▍         | 18537/414113 [00:03<01:05, 6035.95it/s]\u001b[A\n",
      "  5%|▍         | 19141/414113 [00:03<01:05, 6030.59it/s]\u001b[A\n",
      "  5%|▍         | 19761/414113 [00:03<01:04, 6079.16it/s]\u001b[A\n",
      "  5%|▍         | 20370/414113 [00:03<01:06, 5952.59it/s]\u001b[A\n",
      "  5%|▌         | 20966/414113 [00:03<01:06, 5949.72it/s]\u001b[A\n",
      "  5%|▌         | 21575/414113 [00:03<01:05, 5989.23it/s]\u001b[A\n",
      "  7%|▋         | 28076/414113 [00:20<04:36, 1396.63it/s]\u001b[A\n",
      "  5%|▌         | 22770/414113 [00:03<01:06, 5927.28it/s]\u001b[A\n",
      "  6%|▌         | 23382/414113 [00:03<01:05, 5983.17it/s]\u001b[A\n",
      "  6%|▌         | 23981/414113 [00:04<01:05, 5921.49it/s]\u001b[A\n",
      "  6%|▌         | 24574/414113 [00:04<01:05, 5921.07it/s]\u001b[A\n",
      "  6%|▌         | 25178/414113 [00:04<01:05, 5955.52it/s]\u001b[A\n",
      "  6%|▌         | 25786/414113 [00:04<01:04, 5989.98it/s]\u001b[A\n",
      "  6%|▋         | 26398/414113 [00:04<01:04, 6028.15it/s]\u001b[A\n",
      "  7%|▋         | 27002/414113 [00:04<01:04, 5988.57it/s]\u001b[A\n",
      "  7%|▋         | 27603/414113 [00:04<01:04, 5993.42it/s]\u001b[A\n",
      "  7%|▋         | 28206/414113 [00:04<01:04, 6001.91it/s]\u001b[A\n",
      "  7%|▋         | 28810/414113 [00:04<01:04, 6011.63it/s]\u001b[A\n",
      "  7%|▋         | 29416/414113 [00:04<01:03, 6025.44it/s]\u001b[A\n",
      "  7%|▋         | 30028/414113 [00:05<01:03, 6051.81it/s]\u001b[A\n",
      "  7%|▋         | 30643/414113 [00:05<01:03, 6079.51it/s]\u001b[A\n",
      "  8%|▊         | 31257/414113 [00:05<01:02, 6095.10it/s]\u001b[A\n",
      "  8%|▊         | 31867/414113 [00:05<01:02, 6096.43it/s]\u001b[A\n",
      "  8%|▊         | 32477/414113 [00:05<01:02, 6080.48it/s]\u001b[A\n",
      "  8%|▊         | 33086/414113 [00:05<01:03, 6011.92it/s]\u001b[A\n",
      "  8%|▊         | 33688/414113 [00:05<01:04, 5889.48it/s]\u001b[A\n",
      "  8%|▊         | 34278/414113 [00:05<01:04, 5891.56it/s]\u001b[A\n",
      "  8%|▊         | 34877/414113 [00:05<01:04, 5918.62it/s]\u001b[A\n",
      "  9%|▊         | 35473/414113 [00:05<01:03, 5929.66it/s]\u001b[A\n",
      "  9%|▊         | 36067/414113 [00:06<01:05, 5798.30it/s]\u001b[A\n",
      "  9%|▉         | 36662/414113 [00:06<01:04, 5841.74it/s]\u001b[A\n",
      "  9%|▉         | 37267/414113 [00:06<01:03, 5902.30it/s]\u001b[A\n",
      "  9%|▉         | 37878/414113 [00:06<01:03, 5962.70it/s]\u001b[A\n",
      "  9%|▉         | 38475/414113 [00:06<01:03, 5938.29it/s]\u001b[A\n",
      "  9%|▉         | 39078/414113 [00:06<01:02, 5963.28it/s]\u001b[A\n",
      " 10%|▉         | 39675/414113 [00:06<01:02, 5960.62it/s]\u001b[A\n",
      " 10%|▉         | 40272/414113 [00:06<01:02, 5958.83it/s]\u001b[A\n",
      " 10%|▉         | 40871/414113 [00:06<01:02, 5968.09it/s]\u001b[A\n",
      " 10%|█         | 41473/414113 [00:06<01:02, 5982.20it/s]\u001b[A\n",
      " 10%|█         | 42072/414113 [00:07<01:02, 5969.19it/s]\u001b[A\n",
      " 10%|█         | 42669/414113 [00:07<01:02, 5922.26it/s]\u001b[A\n",
      " 10%|█         | 43265/414113 [00:07<01:02, 5933.50it/s]\u001b[A\n",
      " 11%|█         | 43869/414113 [00:07<01:02, 5965.05it/s]\u001b[A\n",
      " 11%|█         | 44469/414113 [00:07<01:01, 5972.53it/s]\u001b[A\n",
      " 11%|█         | 45067/414113 [00:07<01:01, 5967.95it/s]\u001b[A\n",
      " 11%|█         | 45679/414113 [00:07<01:01, 6012.55it/s]\u001b[A\n",
      " 11%|█         | 46287/414113 [00:07<01:00, 6032.55it/s]\u001b[A\n",
      " 11%|█▏        | 46891/414113 [00:07<01:01, 5996.71it/s]\u001b[A\n",
      " 11%|█▏        | 47491/414113 [00:07<01:01, 5991.26it/s]\u001b[A\n",
      " 12%|█▏        | 48098/414113 [00:08<01:00, 6014.34it/s]\u001b[A\n",
      " 12%|█▏        | 48708/414113 [00:08<01:00, 6038.35it/s]\u001b[A\n",
      " 12%|█▏        | 49317/414113 [00:08<01:00, 6051.60it/s]\u001b[A\n",
      " 12%|█▏        | 49933/414113 [00:08<00:59, 6083.48it/s]\u001b[A\n",
      " 12%|█▏        | 50551/414113 [00:08<00:59, 6111.91it/s]\u001b[A\n",
      " 12%|█▏        | 51163/414113 [00:08<00:59, 6092.63it/s]\u001b[A\n",
      " 13%|█▎        | 51773/414113 [00:08<00:59, 6066.99it/s]\u001b[A\n",
      " 13%|█▎        | 52380/414113 [00:08<01:00, 5950.12it/s]\u001b[A\n",
      " 13%|█▎        | 52986/414113 [00:08<01:00, 5980.17it/s]\u001b[A\n",
      " 13%|█▎        | 53585/414113 [00:08<01:00, 5949.15it/s]\u001b[A\n",
      " 13%|█▎        | 54196/414113 [00:09<01:00, 5994.01it/s]\u001b[A\n",
      " 13%|█▎        | 54804/414113 [00:09<00:59, 6017.51it/s]\u001b[A\n",
      " 13%|█▎        | 55406/414113 [00:09<00:59, 6017.37it/s]\u001b[A\n",
      " 14%|█▎        | 56012/414113 [00:09<00:59, 6027.66it/s]\u001b[A\n",
      " 14%|█▎        | 56627/414113 [00:09<00:58, 6061.23it/s]\u001b[A\n",
      " 14%|█▍        | 57246/414113 [00:09<00:58, 6098.37it/s]\u001b[A\n",
      " 14%|█▍        | 57867/414113 [00:09<00:58, 6131.02it/s]\u001b[A\n",
      " 14%|█▍        | 58488/414113 [00:09<00:57, 6151.76it/s]\u001b[A\n",
      " 14%|█▍        | 59104/414113 [00:09<00:58, 6059.81it/s]\u001b[A\n",
      " 14%|█▍        | 59714/414113 [00:09<00:58, 6070.89it/s]\u001b[A\n",
      " 15%|█▍        | 60323/414113 [00:10<00:58, 6074.76it/s]\u001b[A\n",
      " 15%|█▍        | 60931/414113 [00:10<00:58, 6068.38it/s]\u001b[A\n",
      " 15%|█▍        | 61539/414113 [00:10<00:58, 6068.39it/s]\u001b[A\n",
      " 15%|█▌        | 62146/414113 [00:10<00:58, 6040.52it/s]\u001b[A\n",
      " 15%|█▌        | 62751/414113 [00:10<00:58, 6000.51it/s]\u001b[A\n",
      " 15%|█▌        | 63352/414113 [00:10<00:58, 5980.10it/s]\u001b[A\n",
      " 15%|█▌        | 63954/414113 [00:10<00:58, 5990.03it/s]\u001b[A\n",
      " 16%|█▌        | 64578/414113 [00:10<00:57, 6059.54it/s]\u001b[A\n",
      " 16%|█▌        | 65191/414113 [00:10<00:57, 6079.78it/s]\u001b[A\n",
      " 16%|█▌        | 65800/414113 [00:10<00:58, 5950.43it/s]\u001b[A\n",
      " 16%|█▌        | 66414/414113 [00:11<00:57, 6003.38it/s]\u001b[A\n",
      " 16%|█▌        | 67034/414113 [00:11<00:57, 6059.55it/s]\u001b[A\n",
      " 16%|█▋        | 67641/414113 [00:11<00:57, 6053.97it/s]\u001b[A\n",
      " 16%|█▋        | 68247/414113 [00:11<00:57, 6031.35it/s]\u001b[A\n",
      " 17%|█▋        | 68860/414113 [00:11<00:56, 6060.36it/s]\u001b[A\n",
      " 17%|█▋        | 69467/414113 [00:11<00:56, 6059.12it/s]\u001b[A\n",
      " 17%|█▋        | 70074/414113 [00:11<00:57, 6023.25it/s]\u001b[A\n",
      " 17%|█▋        | 70684/414113 [00:11<00:56, 6044.85it/s]\u001b[A\n",
      " 17%|█▋        | 71294/414113 [00:11<00:56, 6059.40it/s]\u001b[A\n",
      " 17%|█▋        | 71901/414113 [00:11<00:56, 6045.74it/s]\u001b[A\n",
      " 18%|█▊        | 72506/414113 [00:12<00:56, 6004.73it/s]\u001b[A\n",
      " 18%|█▊        | 73112/414113 [00:12<00:56, 6020.81it/s]\u001b[A\n",
      " 18%|█▊        | 73717/414113 [00:12<00:56, 6027.06it/s]\u001b[A\n",
      " 18%|█▊        | 74320/414113 [00:12<00:56, 6025.95it/s]\u001b[A\n",
      " 18%|█▊        | 74923/414113 [00:12<00:56, 6022.78it/s]\u001b[A\n",
      " 18%|█▊        | 75526/414113 [00:12<00:56, 6022.60it/s]\u001b[A\n",
      " 18%|█▊        | 76129/414113 [00:12<00:56, 5985.15it/s]\u001b[A\n",
      " 19%|█▊        | 76728/414113 [00:13<01:33, 3595.50it/s]\u001b[A\n",
      " 19%|█▊        | 77342/414113 [00:13<01:22, 4105.96it/s]\u001b[A\n",
      " 19%|█▉        | 77956/414113 [00:13<01:13, 4557.70it/s]\u001b[A\n",
      " 19%|█▉        | 78565/414113 [00:13<01:08, 4929.41it/s]\u001b[A\n",
      " 19%|█▉        | 79183/414113 [00:13<01:03, 5247.42it/s]\u001b[A\n",
      " 19%|█▉        | 79808/414113 [00:13<01:00, 5511.75it/s]\u001b[A\n",
      " 19%|█▉        | 80410/414113 [00:13<00:59, 5654.70it/s]\u001b[A\n",
      " 20%|█▉        | 81028/414113 [00:13<00:57, 5800.08it/s]\u001b[A\n",
      " 20%|█▉        | 81631/414113 [00:13<00:56, 5860.69it/s]\u001b[A\n",
      " 20%|█▉        | 82249/414113 [00:13<00:55, 5951.96it/s]\u001b[A\n",
      " 20%|██        | 82856/414113 [00:14<00:55, 5961.29it/s]\u001b[A\n",
      " 20%|██        | 83461/414113 [00:14<00:55, 5978.68it/s]\u001b[A\n",
      " 20%|██        | 84065/414113 [00:14<00:55, 5989.47it/s]\u001b[A\n",
      " 20%|██        | 84668/414113 [00:14<00:55, 5966.41it/s]\u001b[A\n",
      " 21%|██        | 85284/414113 [00:14<00:54, 6020.16it/s]\u001b[A\n",
      " 21%|██        | 85889/414113 [00:14<00:54, 6024.48it/s]\u001b[A\n",
      " 21%|██        | 86493/414113 [00:14<00:54, 6026.16it/s]\u001b[A\n",
      " 21%|██        | 87109/414113 [00:14<00:53, 6062.57it/s]\u001b[A\n",
      " 21%|██        | 87721/414113 [00:14<00:53, 6078.59it/s]\u001b[A\n",
      " 21%|██▏       | 88345/414113 [00:14<00:53, 6123.75it/s]\u001b[A\n",
      " 21%|██▏       | 88960/414113 [00:15<00:53, 6130.30it/s]\u001b[A\n",
      " 22%|██▏       | 89578/414113 [00:15<00:52, 6144.33it/s]\u001b[A\n",
      " 22%|██▏       | 90193/414113 [00:15<00:52, 6121.42it/s]\u001b[A\n",
      " 22%|██▏       | 90811/414113 [00:15<00:52, 6136.84it/s]\u001b[A\n",
      " 22%|██▏       | 91437/414113 [00:15<00:52, 6171.83it/s]\u001b[A\n",
      " 22%|██▏       | 92055/414113 [00:15<00:53, 6026.06it/s]\u001b[A\n",
      " 22%|██▏       | 92683/414113 [00:15<00:52, 6098.42it/s]\u001b[A\n",
      " 23%|██▎       | 93307/414113 [00:15<00:52, 6137.86it/s]\u001b[A\n",
      " 23%|██▎       | 93922/414113 [00:15<00:52, 6100.65it/s]\u001b[A\n",
      " 23%|██▎       | 94535/414113 [00:15<00:52, 6107.40it/s]\u001b[A\n",
      " 23%|██▎       | 95147/414113 [00:16<00:52, 6099.73it/s]\u001b[A\n",
      " 23%|██▎       | 95758/414113 [00:16<00:52, 6088.19it/s]\u001b[A\n",
      " 23%|██▎       | 96367/414113 [00:16<00:52, 6086.78it/s]\u001b[A\n",
      " 23%|██▎       | 96976/414113 [00:16<00:52, 6067.66it/s]\u001b[A\n",
      " 24%|██▎       | 97586/414113 [00:16<00:52, 6075.99it/s]\u001b[A\n",
      " 24%|██▎       | 98194/414113 [00:16<00:52, 6070.74it/s]\u001b[A\n",
      " 24%|██▍       | 98802/414113 [00:16<00:52, 6058.32it/s]\u001b[A\n",
      " 24%|██▍       | 99408/414113 [00:16<00:52, 6037.33it/s]\u001b[A\n",
      " 24%|██▍       | 100012/414113 [00:16<00:52, 5991.57it/s]\u001b[A\n",
      " 24%|██▍       | 100612/414113 [00:16<00:52, 5960.30it/s]\u001b[A\n",
      " 24%|██▍       | 101209/414113 [00:17<00:52, 5945.48it/s]\u001b[A\n",
      " 25%|██▍       | 101823/414113 [00:17<00:52, 6000.99it/s]\u001b[A\n",
      " 25%|██▍       | 102436/414113 [00:17<00:51, 6038.08it/s]\u001b[A\n",
      " 25%|██▍       | 103048/414113 [00:17<00:51, 6061.71it/s]\u001b[A\n",
      " 25%|██▌       | 103660/414113 [00:17<00:51, 6076.97it/s]\u001b[A\n",
      " 25%|██▌       | 104268/414113 [00:17<00:51, 6039.65it/s]\u001b[A\n",
      " 25%|██▌       | 104873/414113 [00:17<00:51, 6014.35it/s]\u001b[A\n",
      " 25%|██▌       | 105475/414113 [00:17<00:51, 5992.43it/s]\u001b[A\n",
      " 26%|██▌       | 106075/414113 [00:17<00:51, 5977.36it/s]\u001b[A\n",
      " 26%|██▌       | 106679/414113 [00:17<00:51, 5993.20it/s]\u001b[A\n",
      " 26%|██▌       | 107283/414113 [00:18<00:51, 6006.24it/s]\u001b[A\n",
      " 26%|██▌       | 107899/414113 [00:18<00:50, 6050.25it/s]\u001b[A\n",
      " 26%|██▌       | 108514/414113 [00:18<00:50, 6077.95it/s]\u001b[A\n",
      " 26%|██▋       | 109127/414113 [00:18<00:50, 6092.72it/s]\u001b[A\n",
      " 26%|██▋       | 109737/414113 [00:18<00:50, 6066.27it/s]\u001b[A\n",
      " 27%|██▋       | 110349/414113 [00:18<00:49, 6077.43it/s]\u001b[A\n",
      " 27%|██▋       | 110957/414113 [00:18<00:50, 5994.51it/s]\u001b[A\n",
      " 27%|██▋       | 111557/414113 [00:18<00:50, 5995.00it/s]\u001b[A\n",
      " 27%|██▋       | 112157/414113 [00:18<00:50, 5932.39it/s]\u001b[A\n",
      " 27%|██▋       | 112751/414113 [00:18<00:50, 5915.40it/s]\u001b[A\n",
      " 27%|██▋       | 113343/414113 [00:19<00:51, 5887.18it/s]\u001b[A\n",
      " 28%|██▊       | 113936/414113 [00:19<00:50, 5898.15it/s]\u001b[A\n",
      " 28%|██▊       | 114526/414113 [00:19<00:50, 5889.17it/s]\u001b[A\n",
      " 28%|██▊       | 115116/414113 [00:19<00:51, 5850.62it/s]\u001b[A\n",
      " 28%|██▊       | 115713/414113 [00:19<00:50, 5883.85it/s]\u001b[A\n",
      " 28%|██▊       | 116302/414113 [00:19<00:52, 5712.91it/s]\u001b[A\n",
      " 28%|██▊       | 116900/414113 [00:19<00:51, 5788.65it/s]\u001b[A\n",
      " 28%|██▊       | 117499/414113 [00:19<00:50, 5845.55it/s]\u001b[A\n",
      " 29%|██▊       | 118097/414113 [00:19<00:50, 5883.81it/s]\u001b[A\n",
      " 29%|██▊       | 118695/414113 [00:19<00:49, 5910.39it/s]\u001b[A\n",
      " 29%|██▉       | 119307/414113 [00:20<00:49, 5968.23it/s]\u001b[A\n",
      " 29%|██▉       | 119910/414113 [00:20<00:49, 5984.66it/s]\u001b[A\n",
      " 29%|██▉       | 120522/414113 [00:20<00:48, 6024.40it/s]\u001b[A\n",
      " 29%|██▉       | 121126/414113 [00:20<00:48, 6026.43it/s]\u001b[A\n",
      " 29%|██▉       | 121729/414113 [00:20<00:48, 6027.29it/s]\u001b[A\n",
      " 30%|██▉       | 122332/414113 [00:20<00:48, 5984.92it/s]\u001b[A\n",
      " 30%|██▉       | 122938/414113 [00:20<00:48, 6006.27it/s]\u001b[A\n",
      " 30%|██▉       | 123539/414113 [00:20<00:48, 5991.42it/s]\u001b[A\n",
      " 30%|██▉       | 124158/414113 [00:20<00:47, 6048.50it/s]\u001b[A\n",
      " 30%|███       | 124764/414113 [00:20<00:48, 6020.57it/s]\u001b[A\n",
      " 30%|███       | 125379/414113 [00:21<00:47, 6057.36it/s]\u001b[A\n",
      " 30%|███       | 125985/414113 [00:21<00:48, 5993.15it/s]\u001b[A\n",
      " 31%|███       | 126585/414113 [00:21<00:48, 5973.08it/s]\u001b[A\n",
      " 31%|███       | 127200/414113 [00:21<00:47, 6023.09it/s]\u001b[A\n",
      " 31%|███       | 127811/414113 [00:21<00:47, 6048.73it/s]\u001b[A\n",
      " 31%|███       | 128427/414113 [00:21<00:46, 6081.11it/s]\u001b[A\n",
      " 31%|███       | 129036/414113 [00:21<00:46, 6073.72it/s]\u001b[A\n",
      " 31%|███▏      | 129644/414113 [00:21<00:46, 6069.32it/s]\u001b[A\n",
      " 31%|███▏      | 130252/414113 [00:21<00:46, 6071.19it/s]\u001b[A\n",
      " 32%|███▏      | 130860/414113 [00:21<00:46, 6069.45it/s]\u001b[A\n",
      " 32%|███▏      | 131473/414113 [00:22<00:46, 6085.14it/s]\u001b[A\n",
      " 32%|███▏      | 132082/414113 [00:22<00:46, 6069.28it/s]\u001b[A\n",
      " 32%|███▏      | 132689/414113 [00:22<00:46, 6036.81it/s]\u001b[A\n",
      " 32%|███▏      | 133302/414113 [00:22<00:46, 6064.18it/s]\u001b[A\n",
      " 32%|███▏      | 133909/414113 [00:22<00:46, 6022.52it/s]\u001b[A\n",
      " 32%|███▏      | 134512/414113 [00:22<00:47, 5944.04it/s]\u001b[A\n",
      " 33%|███▎      | 135118/414113 [00:22<00:46, 5976.88it/s]\u001b[A\n",
      " 33%|███▎      | 135726/414113 [00:22<00:46, 6006.25it/s]\u001b[A\n",
      " 33%|███▎      | 136327/414113 [00:22<00:46, 5959.51it/s]\u001b[A\n",
      " 33%|███▎      | 136924/414113 [00:23<00:47, 5855.66it/s]\u001b[A\n",
      " 33%|███▎      | 137532/414113 [00:23<00:46, 5919.18it/s]\u001b[A\n",
      " 33%|███▎      | 138140/414113 [00:23<00:46, 5965.32it/s]\u001b[A\n",
      " 34%|███▎      | 138738/414113 [00:23<00:46, 5882.34it/s]\u001b[A\n",
      " 34%|███▎      | 139341/414113 [00:23<00:46, 5924.01it/s]\u001b[A\n",
      " 34%|███▍      | 139952/414113 [00:23<00:45, 5975.82it/s]\u001b[A\n",
      " 34%|███▍      | 140557/414113 [00:23<00:45, 5995.43it/s]\u001b[A\n",
      " 34%|███▍      | 141173/414113 [00:23<00:45, 6041.52it/s]\u001b[A\n",
      " 34%|███▍      | 141778/414113 [00:23<00:45, 6021.72it/s]\u001b[A\n",
      " 34%|███▍      | 142394/414113 [00:23<00:44, 6062.39it/s]\u001b[A\n",
      " 35%|███▍      | 143001/414113 [00:24<00:45, 6017.23it/s]\u001b[A\n",
      " 35%|███▍      | 143603/414113 [00:24<00:44, 6017.91it/s]\u001b[A\n",
      " 35%|███▍      | 144205/414113 [00:24<00:45, 5994.84it/s]\u001b[A\n",
      " 35%|███▍      | 144818/414113 [00:24<00:44, 6033.14it/s]\u001b[A\n",
      " 35%|███▌      | 145432/414113 [00:24<00:44, 6062.03it/s]\u001b[A\n",
      " 35%|███▌      | 146039/414113 [00:24<00:44, 6060.13it/s]\u001b[A\n",
      " 35%|███▌      | 146649/414113 [00:24<00:44, 6069.81it/s]\u001b[A\n",
      " 36%|███▌      | 147257/414113 [00:24<00:44, 6058.25it/s]\u001b[A\n",
      " 36%|███▌      | 147863/414113 [00:24<00:45, 5910.37it/s]\u001b[A\n",
      " 36%|███▌      | 148467/414113 [00:24<00:44, 5947.96it/s]\u001b[A\n",
      " 36%|███▌      | 149081/414113 [00:25<00:44, 6003.46it/s]\u001b[A\n",
      " 36%|███▌      | 149698/414113 [00:25<00:43, 6050.84it/s]\u001b[A\n",
      " 36%|███▋      | 150305/414113 [00:25<00:43, 6055.55it/s]\u001b[A\n",
      " 36%|███▋      | 150926/414113 [00:25<00:43, 6097.96it/s]\u001b[A\n",
      " 37%|███▋      | 151537/414113 [00:25<00:43, 6052.03it/s]\u001b[A\n",
      " 37%|███▋      | 152143/414113 [00:25<00:43, 6028.93it/s]\u001b[A\n",
      " 37%|███▋      | 152750/414113 [00:25<00:43, 6040.83it/s]\u001b[A\n",
      " 37%|███▋      | 153355/414113 [00:25<00:43, 6020.14it/s]\u001b[A\n",
      " 37%|███▋      | 153966/414113 [00:25<00:43, 6044.71it/s]\u001b[A\n",
      " 37%|███▋      | 154580/414113 [00:25<00:42, 6071.68it/s]\u001b[A\n",
      " 37%|███▋      | 155194/414113 [00:26<00:42, 6090.95it/s]\u001b[A\n",
      " 38%|███▊      | 155804/414113 [00:26<00:42, 6055.52it/s]\u001b[A\n",
      " 38%|███▊      | 156428/414113 [00:26<00:42, 6107.53it/s]\u001b[A\n",
      " 38%|███▊      | 157040/414113 [00:26<00:42, 6110.88it/s]\u001b[A\n",
      " 38%|███▊      | 157653/414113 [00:26<00:41, 6115.51it/s]\u001b[A\n",
      " 38%|███▊      | 158285/414113 [00:26<00:41, 6172.51it/s]\u001b[A\n",
      " 38%|███▊      | 158914/414113 [00:26<00:41, 6206.04it/s]\u001b[A\n",
      " 39%|███▊      | 159535/414113 [00:26<00:41, 6185.57it/s]\u001b[A\n",
      " 39%|███▊      | 160154/414113 [00:26<00:41, 6147.74it/s]\u001b[A\n",
      " 39%|███▉      | 160769/414113 [00:26<00:41, 6089.80it/s]\u001b[A\n",
      " 39%|███▉      | 161381/414113 [00:27<00:41, 6098.30it/s]\u001b[A\n",
      " 39%|███▉      | 161991/414113 [00:27<00:41, 6087.02it/s]\u001b[A\n",
      " 39%|███▉      | 162615/414113 [00:27<00:41, 6130.91it/s]\u001b[A\n",
      " 39%|███▉      | 163233/414113 [00:27<00:40, 6144.12it/s]\u001b[A\n",
      " 40%|███▉      | 163862/414113 [00:27<00:40, 6186.27it/s]\u001b[A\n",
      " 40%|███▉      | 164495/414113 [00:27<00:40, 6226.78it/s]\u001b[A\n",
      " 40%|███▉      | 165130/414113 [00:27<00:39, 6263.02it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 165757/414113 [00:27<00:40, 6186.21it/s]\u001b[A\n",
      " 40%|████      | 166376/414113 [00:27<00:40, 6123.74it/s]\u001b[A\n",
      " 40%|████      | 166989/414113 [00:27<00:40, 6037.70it/s]\u001b[A\n",
      " 40%|████      | 167594/414113 [00:28<00:40, 6039.56it/s]\u001b[A\n",
      " 41%|████      | 168199/414113 [00:28<00:40, 6007.95it/s]\u001b[A\n",
      " 41%|████      | 168812/414113 [00:28<00:40, 6043.40it/s]\u001b[A\n",
      " 41%|████      | 169427/414113 [00:28<00:40, 6073.05it/s]\u001b[A\n",
      " 41%|████      | 170037/414113 [00:28<00:40, 6079.56it/s]\u001b[A\n",
      " 41%|████      | 170647/414113 [00:28<00:40, 6083.79it/s]\u001b[A\n",
      " 41%|████▏     | 171257/414113 [00:28<00:39, 6086.92it/s]\u001b[A\n",
      " 42%|████▏     | 171866/414113 [00:28<00:40, 6055.90it/s]\u001b[A\n",
      " 42%|████▏     | 172473/414113 [00:28<00:39, 6058.07it/s]\u001b[A\n",
      " 42%|████▏     | 173082/414113 [00:28<00:39, 6065.42it/s]\u001b[A\n",
      " 42%|████▏     | 173689/414113 [00:29<00:39, 6036.51it/s]\u001b[A\n",
      " 42%|████▏     | 174293/414113 [00:29<00:39, 6008.55it/s]\u001b[A\n",
      " 42%|████▏     | 174898/414113 [00:29<00:39, 6019.96it/s]\u001b[A\n",
      " 42%|████▏     | 175501/414113 [00:29<00:39, 6017.01it/s]\u001b[A\n",
      " 43%|████▎     | 176106/414113 [00:29<00:39, 6025.24it/s]\u001b[A\n",
      " 43%|████▎     | 176717/414113 [00:29<00:39, 6047.84it/s]\u001b[A\n",
      " 43%|████▎     | 177322/414113 [00:29<01:14, 3187.58it/s]\u001b[A\n",
      " 43%|████▎     | 177923/414113 [00:30<01:03, 3709.82it/s]\u001b[A\n",
      " 43%|████▎     | 178528/414113 [00:30<00:56, 4196.26it/s]\u001b[A\n",
      " 43%|████▎     | 179128/414113 [00:30<00:50, 4610.25it/s]\u001b[A\n",
      " 43%|████▎     | 179730/414113 [00:30<00:47, 4958.53it/s]\u001b[A\n",
      " 44%|████▎     | 180322/414113 [00:30<00:44, 5210.95it/s]\u001b[A\n",
      " 44%|████▎     | 180920/414113 [00:30<00:43, 5419.68it/s]\u001b[A\n",
      " 44%|████▍     | 181523/414113 [00:30<00:41, 5588.37it/s]\u001b[A\n",
      " 44%|████▍     | 182132/414113 [00:30<00:40, 5727.14it/s]\u001b[A\n",
      " 44%|████▍     | 182727/414113 [00:30<00:40, 5756.27it/s]\u001b[A\n",
      " 44%|████▍     | 183318/414113 [00:30<00:40, 5701.43it/s]\u001b[A\n",
      " 44%|████▍     | 183899/414113 [00:31<00:40, 5649.89it/s]\u001b[A\n",
      " 45%|████▍     | 184497/414113 [00:31<00:39, 5742.63it/s]\u001b[A\n",
      " 45%|████▍     | 185104/414113 [00:31<00:39, 5835.79it/s]\u001b[A\n",
      " 45%|████▍     | 185714/414113 [00:31<00:38, 5909.85it/s]\u001b[A\n",
      " 45%|████▍     | 186336/414113 [00:31<00:37, 5997.19it/s]\u001b[A\n",
      " 45%|████▌     | 186939/414113 [00:31<00:37, 6006.97it/s]\u001b[A\n",
      " 45%|████▌     | 187562/414113 [00:31<00:37, 6070.21it/s]\u001b[A\n",
      " 45%|████▌     | 188171/414113 [00:31<00:37, 5948.21it/s]\u001b[A\n",
      " 46%|████▌     | 188782/414113 [00:31<00:37, 5993.55it/s]\u001b[A\n",
      " 46%|████▌     | 189383/414113 [00:31<00:37, 5997.51it/s]\u001b[A\n",
      " 46%|████▌     | 189984/414113 [00:32<00:37, 5996.52it/s]\u001b[A\n",
      " 46%|████▌     | 190589/414113 [00:32<00:37, 6011.51it/s]\u001b[A\n",
      " 46%|████▌     | 191191/414113 [00:32<00:37, 5986.38it/s]\u001b[A\n",
      " 46%|████▋     | 191800/414113 [00:32<00:36, 6016.14it/s]\u001b[A\n",
      " 46%|████▋     | 192439/414113 [00:32<00:36, 6121.54it/s]\u001b[A\n",
      " 47%|████▋     | 193053/414113 [00:32<00:36, 6123.80it/s]\u001b[A\n",
      " 47%|████▋     | 193666/414113 [00:32<00:36, 6056.03it/s]\u001b[A\n",
      " 47%|████▋     | 194276/414113 [00:32<00:36, 6066.62it/s]\u001b[A\n",
      " 47%|████▋     | 194884/414113 [00:32<00:36, 6036.44it/s]\u001b[A\n",
      " 47%|████▋     | 195498/414113 [00:33<00:36, 6065.79it/s]\u001b[A\n",
      " 47%|████▋     | 196109/414113 [00:33<00:35, 6076.89it/s]\u001b[A\n",
      " 48%|████▊     | 196717/414113 [00:33<00:36, 5993.10it/s]\u001b[A\n",
      " 48%|████▊     | 197323/414113 [00:33<00:36, 6011.84it/s]\u001b[A\n",
      " 48%|████▊     | 197935/414113 [00:33<00:35, 6043.76it/s]\u001b[A\n",
      " 48%|████▊     | 198565/414113 [00:33<00:35, 6115.72it/s]\u001b[A\n",
      " 48%|████▊     | 199198/414113 [00:33<00:34, 6177.45it/s]\u001b[A\n",
      " 48%|████▊     | 199820/414113 [00:33<00:34, 6187.26it/s]\u001b[A\n",
      " 48%|████▊     | 200449/414113 [00:33<00:34, 6217.35it/s]\u001b[A\n",
      " 49%|████▊     | 201071/414113 [00:33<00:34, 6192.58it/s]\u001b[A\n",
      " 49%|████▊     | 201693/414113 [00:34<00:34, 6200.15it/s]\u001b[A\n",
      " 49%|████▉     | 202314/414113 [00:34<00:34, 6136.29it/s]\u001b[A\n",
      " 49%|████▉     | 202931/414113 [00:34<00:34, 6142.80it/s]\u001b[A\n",
      " 49%|████▉     | 203546/414113 [00:34<00:34, 6139.48it/s]\u001b[A\n",
      " 49%|████▉     | 204163/414113 [00:34<00:34, 6145.93it/s]\u001b[A\n",
      " 49%|████▉     | 204785/414113 [00:34<00:33, 6167.43it/s]\u001b[A\n",
      " 50%|████▉     | 205404/414113 [00:34<00:33, 6171.22it/s]\u001b[A\n",
      " 50%|████▉     | 206022/414113 [00:34<00:33, 6139.70it/s]\u001b[A\n",
      " 50%|████▉     | 206637/414113 [00:34<00:34, 6097.45it/s]\u001b[A\n",
      " 50%|█████     | 207247/414113 [00:34<00:33, 6087.71it/s]\u001b[A\n",
      " 50%|█████     | 207883/414113 [00:35<00:33, 6166.42it/s]\u001b[A\n",
      " 50%|█████     | 208501/414113 [00:35<00:33, 6167.68it/s]\u001b[A\n",
      " 50%|█████     | 209122/414113 [00:35<00:33, 6180.21it/s]\u001b[A\n",
      " 51%|█████     | 209745/414113 [00:35<00:32, 6194.26it/s]\u001b[A\n",
      " 51%|█████     | 210365/414113 [00:35<00:33, 6126.18it/s]\u001b[A\n",
      " 51%|█████     | 210978/414113 [00:35<00:33, 6117.67it/s]\u001b[A\n",
      " 51%|█████     | 211598/414113 [00:35<00:32, 6142.13it/s]\u001b[A\n",
      " 51%|█████     | 212223/414113 [00:35<00:32, 6172.17it/s]\u001b[A\n",
      " 51%|█████▏    | 212841/414113 [00:35<00:32, 6109.49it/s]\u001b[A\n",
      " 52%|█████▏    | 213459/414113 [00:35<00:32, 6130.46it/s]\u001b[A\n",
      " 52%|█████▏    | 214073/414113 [00:36<00:32, 6096.33it/s]\u001b[A\n",
      " 52%|█████▏    | 214683/414113 [00:36<00:33, 5974.78it/s]\u001b[A\n",
      " 52%|█████▏    | 215298/414113 [00:36<00:32, 6026.17it/s]\u001b[A\n",
      " 52%|█████▏    | 215902/414113 [00:36<00:32, 6028.01it/s]\u001b[A\n",
      " 52%|█████▏    | 216523/414113 [00:36<00:32, 6079.09it/s]\u001b[A\n",
      " 52%|█████▏    | 217135/414113 [00:36<00:32, 6087.69it/s]\u001b[A\n",
      " 53%|█████▎    | 217757/414113 [00:36<00:32, 6126.27it/s]\u001b[A\n",
      " 53%|█████▎    | 218382/414113 [00:36<00:31, 6127.65it/s]\u001b[A\n",
      " 53%|█████▎    | 219003/414113 [00:36<00:31, 6151.24it/s]\u001b[A\n",
      " 53%|█████▎    | 219627/414113 [00:36<00:31, 6177.41it/s]\u001b[A\n",
      " 53%|█████▎    | 220245/414113 [00:37<00:31, 6147.42it/s]\u001b[A\n",
      " 53%|█████▎    | 220871/414113 [00:37<00:31, 6177.93it/s]\u001b[A\n",
      " 53%|█████▎    | 221500/414113 [00:37<00:31, 6208.96it/s]\u001b[A\n",
      " 54%|█████▎    | 222122/414113 [00:37<00:31, 6183.64it/s]\u001b[A\n",
      " 54%|█████▍    | 222741/414113 [00:37<00:31, 6169.81it/s]\u001b[A\n",
      " 54%|█████▍    | 223361/414113 [00:37<00:30, 6178.57it/s]\u001b[A\n",
      " 54%|█████▍    | 223979/414113 [00:37<00:30, 6153.31it/s]\u001b[A\n",
      " 54%|█████▍    | 224595/414113 [00:37<00:30, 6148.96it/s]\u001b[A\n",
      " 54%|█████▍    | 225214/414113 [00:37<00:30, 6158.94it/s]\u001b[A\n",
      " 55%|█████▍    | 225830/414113 [00:37<00:30, 6118.42it/s]\u001b[A\n",
      " 55%|█████▍    | 226452/414113 [00:38<00:30, 6145.84it/s]\u001b[A\n",
      " 55%|█████▍    | 227067/414113 [00:38<00:30, 6134.87it/s]\u001b[A\n",
      " 55%|█████▍    | 227720/414113 [00:38<00:29, 6246.50it/s]\u001b[A\n",
      " 55%|█████▌    | 228346/414113 [00:38<00:29, 6240.42it/s]\u001b[A\n",
      " 55%|█████▌    | 228971/414113 [00:38<00:29, 6228.27it/s]\u001b[A\n",
      " 55%|█████▌    | 229595/414113 [00:38<00:29, 6191.88it/s]\u001b[A\n",
      " 56%|█████▌    | 230215/414113 [00:38<00:29, 6145.37it/s]\u001b[A\n",
      " 56%|█████▌    | 230830/414113 [00:38<00:29, 6115.99it/s]\u001b[A\n",
      " 56%|█████▌    | 231442/414113 [00:38<00:29, 6100.30it/s]\u001b[A\n",
      " 56%|█████▌    | 232053/414113 [00:38<00:29, 6076.90it/s]\u001b[A\n",
      " 56%|█████▌    | 232661/414113 [00:39<00:29, 6057.66it/s]\u001b[A\n",
      " 56%|█████▋    | 233275/414113 [00:39<00:29, 6079.72it/s]\u001b[A\n",
      " 56%|█████▋    | 233884/414113 [00:39<00:29, 6081.63it/s]\u001b[A\n",
      " 57%|█████▋    | 234494/414113 [00:39<00:29, 6082.83it/s]\u001b[A\n",
      " 57%|█████▋    | 235103/414113 [00:39<00:29, 6045.34it/s]\u001b[A\n",
      " 57%|█████▋    | 235716/414113 [00:39<00:29, 6067.74it/s]\u001b[A\n",
      " 57%|█████▋    | 236330/414113 [00:39<00:29, 6086.59it/s]\u001b[A\n",
      " 57%|█████▋    | 236939/414113 [00:39<00:29, 6083.60it/s]\u001b[A\n",
      " 57%|█████▋    | 237554/414113 [00:39<00:28, 6102.93it/s]\u001b[A\n",
      " 58%|█████▊    | 238165/414113 [00:39<00:28, 6092.35it/s]\u001b[A\n",
      " 58%|█████▊    | 238775/414113 [00:40<00:28, 6071.37it/s]\u001b[A\n",
      " 58%|█████▊    | 239401/414113 [00:40<00:28, 6125.70it/s]\u001b[A\n",
      " 58%|█████▊    | 240016/414113 [00:40<00:28, 6131.69it/s]\u001b[A\n",
      " 58%|█████▊    | 240630/414113 [00:40<00:28, 6091.21it/s]\u001b[A\n",
      " 58%|█████▊    | 241240/414113 [00:40<00:29, 5859.59it/s]\u001b[A\n",
      " 58%|█████▊    | 241855/414113 [00:40<00:28, 5943.09it/s]\u001b[A\n",
      " 59%|█████▊    | 242476/414113 [00:40<00:28, 6019.56it/s]\u001b[A\n",
      " 59%|█████▊    | 243089/414113 [00:40<00:28, 6048.97it/s]\u001b[A\n",
      " 59%|█████▉    | 243706/414113 [00:40<00:28, 6084.46it/s]\u001b[A\n",
      " 59%|█████▉    | 244316/414113 [00:40<00:27, 6082.59it/s]\u001b[A\n",
      " 59%|█████▉    | 244926/414113 [00:41<00:27, 6087.56it/s]\u001b[A\n",
      " 59%|█████▉    | 245542/414113 [00:41<00:27, 6108.78it/s]\u001b[A\n",
      " 59%|█████▉    | 246156/414113 [00:41<00:27, 6114.81it/s]\u001b[A\n",
      " 60%|█████▉    | 246768/414113 [00:41<00:27, 6102.35it/s]\u001b[A\n",
      " 60%|█████▉    | 247384/414113 [00:41<00:27, 6117.71it/s]\u001b[A\n",
      " 60%|█████▉    | 248003/414113 [00:41<00:27, 6137.40it/s]\u001b[A\n",
      " 60%|██████    | 248617/414113 [00:41<00:27, 6093.33it/s]\u001b[A\n",
      " 60%|██████    | 249227/414113 [00:41<00:27, 6076.66it/s]\u001b[A\n",
      " 60%|██████    | 249837/414113 [00:41<00:27, 6081.88it/s]\u001b[A\n",
      " 60%|██████    | 250446/414113 [00:41<00:26, 6073.01it/s]\u001b[A\n",
      " 61%|██████    | 251054/414113 [00:42<00:26, 6073.66it/s]\u001b[A\n",
      " 61%|██████    | 251672/414113 [00:42<00:26, 6102.75it/s]\u001b[A\n",
      " 61%|██████    | 252283/414113 [00:42<00:26, 5998.07it/s]\u001b[A\n",
      " 61%|██████    | 252894/414113 [00:42<00:26, 6029.62it/s]\u001b[A\n",
      " 61%|██████    | 253505/414113 [00:42<00:26, 6051.83it/s]\u001b[A\n",
      " 61%|██████▏   | 254111/414113 [00:42<00:26, 6038.34it/s]\u001b[A\n",
      " 62%|██████▏   | 254716/414113 [00:42<00:26, 5988.85it/s]\u001b[A\n",
      " 62%|██████▏   | 255326/414113 [00:42<00:26, 6019.09it/s]\u001b[A\n",
      " 62%|██████▏   | 255936/414113 [00:42<00:26, 6041.57it/s]\u001b[A\n",
      " 62%|██████▏   | 256550/414113 [00:42<00:25, 6069.55it/s]\u001b[A\n",
      " 62%|██████▏   | 257161/414113 [00:43<00:25, 6079.84it/s]\u001b[A\n",
      " 62%|██████▏   | 257780/414113 [00:43<00:25, 6111.92it/s]\u001b[A\n",
      " 62%|██████▏   | 258392/414113 [00:43<00:25, 6085.04it/s]\u001b[A\n",
      " 63%|██████▎   | 259001/414113 [00:43<00:25, 6055.74it/s]\u001b[A\n",
      " 63%|██████▎   | 259607/414113 [00:43<00:25, 6056.08it/s]\u001b[A\n",
      " 63%|██████▎   | 260213/414113 [00:43<00:25, 6048.71it/s]\u001b[A\n",
      " 63%|██████▎   | 260823/414113 [00:43<00:25, 6063.65it/s]\u001b[A\n",
      " 63%|██████▎   | 261430/414113 [00:43<00:25, 5987.70it/s]\u001b[A\n",
      " 63%|██████▎   | 262030/414113 [00:43<00:25, 5982.28it/s]\u001b[A\n",
      " 63%|██████▎   | 262629/414113 [00:44<00:25, 5982.89it/s]\u001b[A\n",
      " 64%|██████▎   | 263231/414113 [00:44<00:25, 5993.54it/s]\u001b[A\n",
      " 64%|██████▎   | 263845/414113 [00:44<00:24, 6035.26it/s]\u001b[A\n",
      " 64%|██████▍   | 264453/414113 [00:44<00:24, 6047.99it/s]\u001b[A\n",
      " 64%|██████▍   | 265083/414113 [00:44<00:24, 6119.57it/s]\u001b[A\n",
      " 64%|██████▍   | 265703/414113 [00:44<00:24, 6141.07it/s]\u001b[A\n",
      " 64%|██████▍   | 266318/414113 [00:44<00:24, 6115.84it/s]\u001b[A\n",
      " 64%|██████▍   | 266930/414113 [00:44<00:24, 5978.84it/s]\u001b[A\n",
      " 65%|██████▍   | 267557/414113 [00:44<00:24, 6062.94it/s]\u001b[A\n",
      " 65%|██████▍   | 268189/414113 [00:44<00:23, 6134.82it/s]\u001b[A\n",
      " 65%|██████▍   | 268804/414113 [00:45<00:23, 6129.83it/s]\u001b[A\n",
      " 65%|██████▌   | 269432/414113 [00:45<00:23, 6173.31it/s]\u001b[A\n",
      " 65%|██████▌   | 270050/414113 [00:45<00:23, 6165.49it/s]\u001b[A\n",
      " 65%|██████▌   | 270667/414113 [00:45<00:23, 6136.89it/s]\u001b[A\n",
      " 66%|██████▌   | 271281/414113 [00:45<00:23, 6123.72it/s]\u001b[A\n",
      " 66%|██████▌   | 271906/414113 [00:45<00:23, 6159.09it/s]\u001b[A\n",
      " 66%|██████▌   | 272523/414113 [00:45<00:23, 6101.80it/s]\u001b[A\n",
      " 66%|██████▌   | 273134/414113 [00:45<00:23, 6073.77it/s]\u001b[A\n",
      " 66%|██████▌   | 273746/414113 [00:45<00:23, 6086.41it/s]\u001b[A\n",
      " 66%|██████▋   | 274355/414113 [00:45<00:23, 6064.42it/s]\u001b[A\n",
      " 66%|██████▋   | 274962/414113 [00:46<00:22, 6065.29it/s]\u001b[A\n",
      " 67%|██████▋   | 275569/414113 [00:46<00:22, 6061.40it/s]\u001b[A\n",
      " 67%|██████▋   | 276182/414113 [00:46<00:22, 6080.84it/s]\u001b[A\n",
      " 67%|██████▋   | 276791/414113 [00:46<00:22, 6025.70it/s]\u001b[A\n",
      " 67%|██████▋   | 277414/414113 [00:46<00:22, 6083.44it/s]\u001b[A\n",
      " 67%|██████▋   | 278039/414113 [00:46<00:22, 6130.20it/s]\u001b[A\n",
      " 67%|██████▋   | 278653/414113 [00:46<00:22, 6060.68it/s]\u001b[A\n",
      " 67%|██████▋   | 279260/414113 [00:46<00:22, 5981.56it/s]\u001b[A\n",
      " 68%|██████▊   | 279866/414113 [00:46<00:22, 6003.61it/s]\u001b[A\n",
      " 68%|██████▊   | 280484/414113 [00:46<00:22, 6053.37it/s]\u001b[A\n",
      " 68%|██████▊   | 281095/414113 [00:47<00:21, 6068.73it/s]\u001b[A\n",
      " 68%|██████▊   | 281703/414113 [00:47<00:21, 6063.25it/s]\u001b[A\n",
      " 68%|██████▊   | 282310/414113 [00:47<00:21, 6036.65it/s]\u001b[A\n",
      " 68%|██████▊   | 282917/414113 [00:47<00:21, 6044.42it/s]\u001b[A\n",
      " 68%|██████▊   | 283544/414113 [00:47<00:21, 6107.84it/s]\u001b[A\n",
      " 69%|██████▊   | 284158/414113 [00:47<00:21, 6116.38it/s]\u001b[A\n",
      " 69%|██████▉   | 284777/414113 [00:47<00:21, 6136.81it/s]\u001b[A\n",
      " 69%|██████▉   | 285391/414113 [00:47<00:21, 6118.96it/s]\u001b[A\n",
      " 69%|██████▉   | 286004/414113 [00:47<00:20, 6103.13it/s]\u001b[A\n",
      " 69%|██████▉   | 286615/414113 [00:47<00:21, 6024.18it/s]\u001b[A\n",
      " 69%|██████▉   | 287232/414113 [00:48<00:20, 6064.64it/s]\u001b[A\n",
      " 70%|██████▉   | 287856/414113 [00:48<00:20, 6113.74it/s]\u001b[A\n",
      " 70%|██████▉   | 288468/414113 [00:48<00:20, 6083.41it/s]\u001b[A\n",
      " 70%|██████▉   | 289083/414113 [00:48<00:20, 6100.74it/s]\u001b[A\n",
      " 70%|██████▉   | 289706/414113 [00:48<00:20, 6136.89it/s]\u001b[A\n",
      " 70%|███████   | 290324/414113 [00:48<00:20, 6148.92it/s]\u001b[A\n",
      " 70%|███████   | 290940/414113 [00:48<00:20, 6146.71it/s]\u001b[A\n",
      " 70%|███████   | 291555/414113 [00:48<00:20, 6099.04it/s]\u001b[A\n",
      " 71%|███████   | 292173/414113 [00:48<00:19, 6122.37it/s]\u001b[A\n",
      " 71%|███████   | 292786/414113 [00:48<00:19, 6095.27it/s]\u001b[A\n",
      " 71%|███████   | 293400/414113 [00:49<00:19, 6108.30it/s]\u001b[A\n",
      " 71%|███████   | 294013/414113 [00:49<00:19, 6113.11it/s]\u001b[A\n",
      " 71%|███████   | 294625/414113 [00:49<00:19, 6114.57it/s]\u001b[A\n",
      " 71%|███████▏  | 295237/414113 [00:49<00:19, 6092.06it/s]\u001b[A\n",
      " 71%|███████▏  | 295850/414113 [00:49<00:19, 6100.45it/s]\u001b[A\n",
      " 72%|███████▏  | 296461/414113 [00:49<00:19, 6041.40it/s]\u001b[A\n",
      " 72%|███████▏  | 297066/414113 [00:49<00:19, 6029.68it/s]\u001b[A\n",
      " 72%|███████▏  | 297670/414113 [00:49<00:19, 6010.20it/s]\u001b[A\n",
      " 72%|███████▏  | 298281/414113 [00:49<00:19, 6037.54it/s]\u001b[A\n",
      " 72%|███████▏  | 298899/414113 [00:49<00:18, 6077.45it/s]\u001b[A\n",
      " 72%|███████▏  | 299522/414113 [00:50<00:18, 6122.41it/s]\u001b[A\n",
      " 72%|███████▏  | 300142/414113 [00:50<00:18, 6144.91it/s]\u001b[A\n",
      " 73%|███████▎  | 300757/414113 [00:50<00:18, 6145.29it/s]\u001b[A\n",
      " 73%|███████▎  | 301372/414113 [00:50<00:18, 6138.37it/s]\u001b[A\n",
      " 73%|███████▎  | 301990/414113 [00:50<00:18, 6148.28it/s]\u001b[A\n",
      " 73%|███████▎  | 302605/414113 [00:50<00:32, 3478.51it/s]\u001b[A\n",
      " 73%|███████▎  | 303223/414113 [00:50<00:27, 4002.90it/s]\u001b[A\n",
      " 73%|███████▎  | 303828/414113 [00:51<00:24, 4453.76it/s]\u001b[A\n",
      " 74%|███████▎  | 304403/414113 [00:51<00:22, 4776.05it/s]\u001b[A\n",
      " 74%|███████▎  | 305006/414113 [00:51<00:21, 5092.39it/s]\u001b[A\n",
      " 74%|███████▍  | 305624/414113 [00:51<00:20, 5375.19it/s]\u001b[A\n",
      " 74%|███████▍  | 306243/414113 [00:51<00:19, 5594.91it/s]\u001b[A\n",
      " 74%|███████▍  | 306857/414113 [00:51<00:18, 5747.91it/s]\u001b[A\n",
      " 74%|███████▍  | 307468/414113 [00:51<00:18, 5849.97it/s]\u001b[A\n",
      " 74%|███████▍  | 308091/414113 [00:51<00:17, 5958.38it/s]\u001b[A\n",
      " 75%|███████▍  | 308701/414113 [00:51<00:17, 5996.41it/s]\u001b[A\n",
      " 75%|███████▍  | 309312/414113 [00:51<00:17, 6027.03it/s]\u001b[A\n",
      " 75%|███████▍  | 309922/414113 [00:52<00:17, 6007.87it/s]\u001b[A\n",
      " 75%|███████▍  | 310562/414113 [00:52<00:16, 6120.12it/s]\u001b[A\n",
      " 75%|███████▌  | 311180/414113 [00:52<00:16, 6137.51it/s]\u001b[A\n",
      " 75%|███████▌  | 311823/414113 [00:52<00:16, 6219.83it/s]\u001b[A\n",
      " 75%|███████▌  | 312459/414113 [00:52<00:16, 6260.97it/s]\u001b[A\n",
      " 76%|███████▌  | 313087/414113 [00:52<00:16, 6249.19it/s]\u001b[A\n",
      " 76%|███████▌  | 313714/414113 [00:52<00:16, 6203.49it/s]\u001b[A\n",
      " 76%|███████▌  | 314336/414113 [00:52<00:16, 6163.68it/s]\u001b[A\n",
      " 76%|███████▌  | 314954/414113 [00:52<00:16, 6122.36it/s]\u001b[A\n",
      " 76%|███████▌  | 315567/414113 [00:52<00:16, 6093.18it/s]\u001b[A\n",
      " 76%|███████▋  | 316185/414113 [00:53<00:16, 6117.09it/s]\u001b[A\n",
      " 77%|███████▋  | 316813/414113 [00:53<00:15, 6163.23it/s]\u001b[A\n",
      " 77%|███████▋  | 317445/414113 [00:53<00:15, 6207.10it/s]\u001b[A\n",
      " 77%|███████▋  | 318066/414113 [00:53<00:15, 6174.32it/s]\u001b[A\n",
      " 77%|███████▋  | 318684/414113 [00:53<00:15, 6165.87it/s]\u001b[A\n",
      " 77%|███████▋  | 319301/414113 [00:53<00:15, 6123.50it/s]\u001b[A\n",
      " 77%|███████▋  | 319918/414113 [00:53<00:15, 6137.22it/s]\u001b[A\n",
      " 77%|███████▋  | 320534/414113 [00:53<00:15, 6142.88it/s]\u001b[A\n",
      " 78%|███████▊  | 321149/414113 [00:53<00:15, 6120.26it/s]\u001b[A\n",
      " 78%|███████▊  | 321762/414113 [00:53<00:15, 6053.57it/s]\u001b[A\n",
      " 78%|███████▊  | 322368/414113 [00:54<00:15, 6052.49it/s]\u001b[A\n",
      " 78%|███████▊  | 322995/414113 [00:54<00:14, 6115.08it/s]\u001b[A\n",
      " 78%|███████▊  | 323615/414113 [00:54<00:14, 6139.92it/s]\u001b[A\n",
      " 78%|███████▊  | 324230/414113 [00:54<00:14, 6113.71it/s]\u001b[A\n",
      " 78%|███████▊  | 324842/414113 [00:54<00:14, 6094.76it/s]\u001b[A\n",
      " 79%|███████▊  | 325455/414113 [00:54<00:14, 6103.90it/s]\u001b[A\n",
      " 79%|███████▊  | 326068/414113 [00:54<00:14, 6108.83it/s]\u001b[A\n",
      " 79%|███████▉  | 326679/414113 [00:54<00:14, 6098.17it/s]\u001b[A\n",
      " 79%|███████▉  | 327289/414113 [00:54<00:14, 6055.07it/s]\u001b[A\n",
      " 79%|███████▉  | 327895/414113 [00:54<00:14, 6042.55it/s]\u001b[A\n",
      " 79%|███████▉  | 328500/414113 [00:55<00:14, 6031.89it/s]\u001b[A\n",
      " 79%|███████▉  | 329112/414113 [00:55<00:14, 6057.74it/s]\u001b[A\n",
      " 80%|███████▉  | 329718/414113 [00:55<00:13, 6044.80it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 330323/414113 [00:55<00:13, 6034.20it/s]\u001b[A\n",
      " 80%|███████▉  | 330927/414113 [00:55<00:13, 5998.89it/s]\u001b[A\n",
      " 80%|████████  | 331530/414113 [00:55<00:13, 6006.37it/s]\u001b[A\n",
      " 80%|████████  | 332145/414113 [00:55<00:13, 6046.25it/s]\u001b[A\n",
      " 80%|████████  | 332766/414113 [00:55<00:13, 6093.36it/s]\u001b[A\n",
      " 81%|████████  | 333385/414113 [00:55<00:13, 6120.94it/s]\u001b[A\n",
      " 81%|████████  | 333998/414113 [00:55<00:13, 6090.64it/s]\u001b[A\n",
      " 81%|████████  | 334608/414113 [00:56<00:13, 6057.99it/s]\u001b[A\n",
      " 81%|████████  | 335214/414113 [00:56<00:13, 6036.07it/s]\u001b[A\n",
      " 81%|████████  | 335818/414113 [00:56<00:13, 5957.42it/s]\u001b[A\n",
      " 81%|████████  | 336415/414113 [00:56<00:13, 5850.56it/s]\u001b[A\n",
      " 81%|████████▏ | 337010/414113 [00:56<00:13, 5879.22it/s]\u001b[A\n",
      " 82%|████████▏ | 337606/414113 [00:56<00:12, 5903.12it/s]\u001b[A\n",
      " 82%|████████▏ | 338198/414113 [00:56<00:12, 5907.58it/s]\u001b[A\n",
      " 82%|████████▏ | 338790/414113 [00:56<00:12, 5887.33it/s]\u001b[A\n",
      " 82%|████████▏ | 339379/414113 [00:56<00:12, 5875.75it/s]\u001b[A\n",
      " 82%|████████▏ | 339968/414113 [00:56<00:12, 5876.48it/s]\u001b[A\n",
      " 82%|████████▏ | 340577/414113 [00:57<00:12, 5937.33it/s]\u001b[A\n",
      " 82%|████████▏ | 341179/414113 [00:57<00:12, 5960.42it/s]\u001b[A\n",
      " 83%|████████▎ | 341790/414113 [00:57<00:12, 6004.00it/s]\u001b[A\n",
      " 83%|████████▎ | 342391/414113 [00:57<00:12, 5955.86it/s]\u001b[A\n",
      " 83%|████████▎ | 342987/414113 [00:57<00:11, 5949.27it/s]\u001b[A\n",
      " 83%|████████▎ | 343598/414113 [00:57<00:11, 5995.21it/s]\u001b[A\n",
      " 83%|████████▎ | 344198/414113 [00:57<00:11, 5964.66it/s]\u001b[A\n",
      " 83%|████████▎ | 344798/414113 [00:57<00:11, 5972.88it/s]\u001b[A\n",
      " 83%|████████▎ | 345396/414113 [00:57<00:11, 5961.94it/s]\u001b[A\n",
      " 84%|████████▎ | 345993/414113 [00:57<00:11, 5941.14it/s]\u001b[A\n",
      " 84%|████████▎ | 346593/414113 [00:58<00:11, 5958.31it/s]\u001b[A\n",
      " 84%|████████▍ | 347190/414113 [00:58<00:11, 5959.38it/s]\u001b[A\n",
      " 84%|████████▍ | 347794/414113 [00:58<00:11, 5980.31it/s]\u001b[A\n",
      " 84%|████████▍ | 348393/414113 [00:58<00:11, 5972.72it/s]\u001b[A\n",
      " 84%|████████▍ | 349006/414113 [00:58<00:10, 6017.24it/s]\u001b[A\n",
      " 84%|████████▍ | 349629/414113 [00:58<00:10, 6079.41it/s]\u001b[A\n",
      " 85%|████████▍ | 350238/414113 [00:58<00:10, 6064.55it/s]\u001b[A\n",
      " 85%|████████▍ | 350851/414113 [00:58<00:10, 6082.22it/s]\u001b[A\n",
      " 85%|████████▍ | 351460/414113 [00:58<00:10, 6002.26it/s]\u001b[A\n",
      " 85%|████████▌ | 352066/414113 [00:58<00:10, 6016.98it/s]\u001b[A\n",
      " 85%|████████▌ | 352668/414113 [00:59<00:10, 5996.68it/s]\u001b[A\n",
      " 85%|████████▌ | 353271/414113 [00:59<00:10, 6005.30it/s]\u001b[A\n",
      " 85%|████████▌ | 353880/414113 [00:59<00:09, 6029.09it/s]\u001b[A\n",
      " 86%|████████▌ | 354487/414113 [00:59<00:09, 6040.46it/s]\u001b[A\n",
      " 86%|████████▌ | 355100/414113 [00:59<00:09, 6063.95it/s]\u001b[A\n",
      " 86%|████████▌ | 355707/414113 [00:59<00:09, 6057.09it/s]\u001b[A\n",
      " 86%|████████▌ | 356318/414113 [00:59<00:09, 6071.45it/s]\u001b[A\n",
      " 86%|████████▌ | 356926/414113 [00:59<00:09, 6064.63it/s]\u001b[A\n",
      " 86%|████████▋ | 357533/414113 [00:59<00:09, 6035.51it/s]\u001b[A\n",
      " 86%|████████▋ | 358137/414113 [00:59<00:09, 6008.43it/s]\u001b[A\n",
      " 87%|████████▋ | 358745/414113 [01:00<00:09, 6029.71it/s]\u001b[A\n",
      " 87%|████████▋ | 359351/414113 [01:00<00:09, 6036.18it/s]\u001b[A\n",
      " 87%|████████▋ | 359955/414113 [01:00<00:09, 6011.55it/s]\u001b[A\n",
      " 87%|████████▋ | 360557/414113 [01:00<00:08, 6010.74it/s]\u001b[A\n",
      " 87%|████████▋ | 361159/414113 [01:00<00:08, 5957.56it/s]\u001b[A\n",
      " 87%|████████▋ | 361755/414113 [01:00<00:08, 5954.10it/s]\u001b[A\n",
      " 88%|████████▊ | 362354/414113 [01:00<00:08, 5962.35it/s]\u001b[A\n",
      " 88%|████████▊ | 362960/414113 [01:00<00:08, 5990.76it/s]\u001b[A\n",
      " 88%|████████▊ | 363560/414113 [01:00<00:08, 5833.25it/s]\u001b[A\n",
      " 88%|████████▊ | 364166/414113 [01:01<00:08, 5897.43it/s]\u001b[A\n",
      " 88%|████████▊ | 364787/414113 [01:01<00:08, 5984.25it/s]\u001b[A\n",
      " 88%|████████▊ | 365394/414113 [01:01<00:08, 6005.79it/s]\u001b[A\n",
      " 88%|████████▊ | 366001/414113 [01:01<00:07, 6024.19it/s]\u001b[A\n",
      " 89%|████████▊ | 366604/414113 [01:01<00:07, 6018.56it/s]\u001b[A\n",
      " 89%|████████▊ | 367207/414113 [01:01<00:07, 6008.63it/s]\u001b[A\n",
      " 89%|████████▉ | 367817/414113 [01:01<00:07, 6033.40it/s]\u001b[A\n",
      " 89%|████████▉ | 368421/414113 [01:01<00:07, 6000.77it/s]\u001b[A\n",
      " 89%|████████▉ | 369022/414113 [01:01<00:07, 5950.88it/s]\u001b[A\n",
      " 89%|████████▉ | 369618/414113 [01:01<00:07, 5931.85it/s]\u001b[A\n",
      " 89%|████████▉ | 370226/414113 [01:02<00:07, 5975.08it/s]\u001b[A\n",
      " 90%|████████▉ | 370830/414113 [01:02<00:07, 5991.48it/s]\u001b[A\n",
      " 90%|████████▉ | 371430/414113 [01:02<00:07, 5958.28it/s]\u001b[A\n",
      " 90%|████████▉ | 372042/414113 [01:02<00:07, 6004.49it/s]\u001b[A\n",
      " 90%|████████▉ | 372643/414113 [01:02<00:06, 5985.99it/s]\u001b[A\n",
      " 90%|█████████ | 373267/414113 [01:02<00:06, 6059.69it/s]\u001b[A\n",
      " 90%|█████████ | 373886/414113 [01:02<00:06, 6095.70it/s]\u001b[A\n",
      " 90%|█████████ | 374504/414113 [01:02<00:06, 6118.78it/s]\u001b[A\n",
      " 91%|█████████ | 375117/414113 [01:02<00:06, 6087.71it/s]\u001b[A\n",
      " 91%|█████████ | 375726/414113 [01:02<00:06, 6043.25it/s]\u001b[A\n",
      " 91%|█████████ | 376334/414113 [01:03<00:06, 6052.93it/s]\u001b[A\n",
      " 91%|█████████ | 376940/414113 [01:03<00:06, 6034.52it/s]\u001b[A\n",
      " 91%|█████████ | 377544/414113 [01:03<00:06, 5999.90it/s]\u001b[A\n",
      " 91%|█████████▏| 378151/414113 [01:03<00:05, 6020.57it/s]\u001b[A\n",
      " 91%|█████████▏| 378768/414113 [01:03<00:05, 6062.26it/s]\u001b[A\n",
      " 92%|█████████▏| 379375/414113 [01:03<00:05, 6041.24it/s]\u001b[A\n",
      " 92%|█████████▏| 379980/414113 [01:03<00:05, 6018.93it/s]\u001b[A\n",
      " 92%|█████████▏| 380582/414113 [01:03<00:05, 6000.73it/s]\u001b[A\n",
      " 92%|█████████▏| 381188/414113 [01:03<00:05, 6017.91it/s]\u001b[A\n",
      " 92%|█████████▏| 381790/414113 [01:03<00:05, 6011.09it/s]\u001b[A\n",
      " 92%|█████████▏| 382394/414113 [01:04<00:05, 6017.75it/s]\u001b[A\n",
      " 92%|█████████▏| 382996/414113 [01:04<00:05, 5974.50it/s]\u001b[A\n",
      " 93%|█████████▎| 383612/414113 [01:04<00:05, 6025.87it/s]\u001b[A\n",
      " 93%|█████████▎| 384215/414113 [01:04<00:04, 6015.34it/s]\u001b[A\n",
      " 93%|█████████▎| 384817/414113 [01:04<00:04, 6015.89it/s]\u001b[A\n",
      " 93%|█████████▎| 385419/414113 [01:04<00:04, 5938.75it/s]\u001b[A\n",
      " 93%|█████████▎| 386019/414113 [01:04<00:04, 5956.86it/s]\u001b[A\n",
      " 93%|█████████▎| 386632/414113 [01:04<00:04, 6006.06it/s]\u001b[A\n",
      " 94%|█████████▎| 387241/414113 [01:04<00:04, 6029.70it/s]\u001b[A\n",
      " 94%|█████████▎| 387855/414113 [01:04<00:04, 6062.15it/s]\u001b[A\n",
      " 94%|█████████▍| 388464/414113 [01:05<00:04, 6069.39it/s]\u001b[A\n",
      " 94%|█████████▍| 389075/414113 [01:05<00:04, 6078.70it/s]\u001b[A\n",
      " 94%|█████████▍| 389683/414113 [01:05<00:04, 6071.92it/s]\u001b[A\n",
      " 94%|█████████▍| 390291/414113 [01:05<00:03, 6030.80it/s]\u001b[A\n",
      " 94%|█████████▍| 390897/414113 [01:05<00:03, 6038.76it/s]\u001b[A\n",
      " 95%|█████████▍| 391543/414113 [01:05<00:03, 6156.88it/s]\u001b[A\n",
      " 95%|█████████▍| 392167/414113 [01:05<00:03, 6180.41it/s]\u001b[A\n",
      " 95%|█████████▍| 392786/414113 [01:05<00:03, 6161.13it/s]\u001b[A\n",
      " 95%|█████████▍| 393403/414113 [01:05<00:03, 6085.06it/s]\u001b[A\n",
      " 95%|█████████▌| 394012/414113 [01:05<00:03, 5987.30it/s]\u001b[A\n",
      " 95%|█████████▌| 394612/414113 [01:06<00:03, 5988.61it/s]\u001b[A\n",
      " 95%|█████████▌| 395212/414113 [01:06<00:03, 5955.78it/s]\u001b[A\n",
      " 96%|█████████▌| 395808/414113 [01:06<00:03, 5942.13it/s]\u001b[A\n",
      " 96%|█████████▌| 396403/414113 [01:06<00:02, 5909.86it/s]\u001b[A\n",
      " 96%|█████████▌| 397000/414113 [01:06<00:02, 5927.32it/s]\u001b[A\n",
      " 96%|█████████▌| 397593/414113 [01:06<00:02, 5892.17it/s]\u001b[A\n",
      " 96%|█████████▌| 398183/414113 [01:06<00:02, 5891.53it/s]\u001b[A\n",
      " 96%|█████████▋| 398785/414113 [01:06<00:02, 5928.62it/s]\u001b[A\n",
      " 96%|█████████▋| 399390/414113 [01:06<00:02, 5961.23it/s]\u001b[A\n",
      " 97%|█████████▋| 399992/414113 [01:06<00:02, 5975.97it/s]\u001b[A\n",
      " 97%|█████████▋| 400590/414113 [01:07<00:02, 5939.70it/s]\u001b[A\n",
      " 97%|█████████▋| 401188/414113 [01:07<00:02, 5950.30it/s]\u001b[A\n",
      " 97%|█████████▋| 401802/414113 [01:07<00:02, 6003.85it/s]\u001b[A\n",
      " 97%|█████████▋| 402403/414113 [01:07<00:01, 5967.35it/s]\u001b[A\n",
      " 97%|█████████▋| 403020/414113 [01:07<00:01, 6025.45it/s]\u001b[A\n",
      " 97%|█████████▋| 403631/414113 [01:07<00:01, 6050.03it/s]\u001b[A\n",
      " 98%|█████████▊| 404237/414113 [01:07<00:01, 6030.31it/s]\u001b[A\n",
      " 98%|█████████▊| 404841/414113 [01:07<00:01, 5992.74it/s]\u001b[A\n",
      " 98%|█████████▊| 405442/414113 [01:07<00:01, 5996.41it/s]\u001b[A\n",
      " 98%|█████████▊| 406042/414113 [01:07<00:01, 5972.52it/s]\u001b[A\n",
      " 98%|█████████▊| 406640/414113 [01:08<00:01, 5964.60it/s]\u001b[A\n",
      " 98%|█████████▊| 407237/414113 [01:08<00:01, 5954.04it/s]\u001b[A\n",
      " 98%|█████████▊| 407833/414113 [01:08<00:01, 5954.60it/s]\u001b[A\n",
      " 99%|█████████▊| 408434/414113 [01:08<00:00, 5970.72it/s]\u001b[A\n",
      " 99%|█████████▉| 409032/414113 [01:08<00:00, 5958.83it/s]\u001b[A\n",
      " 99%|█████████▉| 409628/414113 [01:08<00:00, 5935.77it/s]\u001b[A\n",
      " 99%|█████████▉| 410232/414113 [01:08<00:00, 5964.51it/s]\u001b[A\n",
      " 99%|█████████▉| 410829/414113 [01:08<00:00, 5870.40it/s]\u001b[A\n",
      " 99%|█████████▉| 411429/414113 [01:08<00:00, 5907.13it/s]\u001b[A\n",
      " 99%|█████████▉| 412021/414113 [01:08<00:00, 5904.10it/s]\u001b[A\n",
      "100%|█████████▉| 412612/414113 [01:09<00:00, 5901.08it/s]\u001b[A\n",
      "100%|█████████▉| 413207/414113 [01:09<00:00, 5912.24it/s]\u001b[A\n",
      "100%|█████████▉| 413804/414113 [01:09<00:00, 5929.22it/s]\u001b[A\n",
      "100%|██████████| 414113/414113 [01:09<00:00, 5972.63it/s]\u001b[ADownloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.torch/models/resnet50-19c8e357.pth\n",
      "\n",
      "  0%|          | 0/102502400 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1056768/102502400 [00:00<00:09, 10560086.55it/s]\u001b[A\n",
      "  7%|▋         | 7266304/102502400 [00:00<00:06, 14041595.31it/s]\u001b[A\n",
      " 11%|█▏        | 11739136/102502400 [00:00<00:05, 17678530.58it/s]\u001b[A\n",
      " 17%|█▋        | 17563648/102502400 [00:00<00:03, 22347468.62it/s]\u001b[A\n",
      " 21%|██        | 21258240/102502400 [00:00<00:03, 24053538.87it/s]\u001b[A\n",
      " 26%|██▌       | 26861568/102502400 [00:00<00:02, 28996656.94it/s]\u001b[A\n",
      " 32%|███▏      | 33169408/102502400 [00:00<00:02, 34603711.02it/s]\u001b[A\n",
      " 38%|███▊      | 38789120/102502400 [00:00<00:01, 39109712.02it/s]\u001b[A\n",
      " 43%|████▎     | 43810816/102502400 [00:00<00:01, 39180942.67it/s]\u001b[A\n",
      " 48%|████▊     | 49012736/102502400 [00:01<00:01, 40514639.72it/s]\u001b[A\n",
      " 54%|█████▍    | 55123968/102502400 [00:01<00:01, 45065878.25it/s]\u001b[A\n",
      " 60%|█████▉    | 61308928/102502400 [00:01<00:00, 49057517.35it/s]\u001b[A\n",
      " 66%|██████▌   | 67608576/102502400 [00:01<00:00, 52538556.54it/s]\u001b[A\n",
      " 74%|███████▎  | 75350016/102502400 [00:01<00:00, 58134028.47it/s]\u001b[A\n",
      " 80%|███████▉  | 81641472/102502400 [00:01<00:00, 48023729.04it/s]\u001b[A\n",
      " 86%|████████▌ | 88047616/102502400 [00:01<00:00, 51921415.67it/s]\u001b[A\n",
      " 91%|█████████▏| 93773824/102502400 [00:01<00:00, 51096834.49it/s]\u001b[A\n",
      " 97%|█████████▋| 99262464/102502400 [00:01<00:00, 51572459.26it/s]\u001b[A\n",
      "100%|██████████| 102502400/102502400 [00:02<00:00, 50577045.04it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "## DONE #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 32        # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 4            # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# DONE #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "\n",
    "# DONE #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr = 0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [100/12942], Loss: 3.8977, Perplexity: 49.2876\n",
      "Epoch [1/4], Step [200/12942], Loss: 3.6714, Perplexity: 39.30834\n",
      "Epoch [1/4], Step [300/12942], Loss: 3.3115, Perplexity: 27.4259\n",
      "Epoch [1/4], Step [400/12942], Loss: 3.4103, Perplexity: 30.27324\n",
      "Epoch [1/4], Step [500/12942], Loss: 3.1962, Perplexity: 24.4396\n",
      "Epoch [1/4], Step [600/12942], Loss: 2.9487, Perplexity: 19.0810\n",
      "Epoch [1/4], Step [700/12942], Loss: 3.0296, Perplexity: 20.6891\n",
      "Epoch [1/4], Step [800/12942], Loss: 3.1471, Perplexity: 23.2675\n",
      "Epoch [1/4], Step [900/12942], Loss: 2.8727, Perplexity: 17.6849\n",
      "Epoch [1/4], Step [1000/12942], Loss: 2.8723, Perplexity: 17.6780\n",
      "Epoch [1/4], Step [1100/12942], Loss: 2.9543, Perplexity: 19.18777\n",
      "Epoch [1/4], Step [1200/12942], Loss: 2.7460, Perplexity: 15.5801\n",
      "Epoch [1/4], Step [1300/12942], Loss: 2.4902, Perplexity: 12.0636\n",
      "Epoch [1/4], Step [1400/12942], Loss: 2.8809, Perplexity: 17.8294\n",
      "Epoch [1/4], Step [1500/12942], Loss: 3.0183, Perplexity: 20.4569\n",
      "Epoch [1/4], Step [1600/12942], Loss: 2.5985, Perplexity: 13.4432\n",
      "Epoch [1/4], Step [1700/12942], Loss: 2.8074, Perplexity: 16.5675\n",
      "Epoch [1/4], Step [1800/12942], Loss: 2.5949, Perplexity: 13.3956\n",
      "Epoch [1/4], Step [1900/12942], Loss: 2.8522, Perplexity: 17.3262\n",
      "Epoch [1/4], Step [2000/12942], Loss: 2.7663, Perplexity: 15.9000\n",
      "Epoch [1/4], Step [2100/12942], Loss: 2.3346, Perplexity: 10.3251\n",
      "Epoch [1/4], Step [2200/12942], Loss: 2.4266, Perplexity: 11.32021\n",
      "Epoch [1/4], Step [2300/12942], Loss: 2.5470, Perplexity: 12.7686\n",
      "Epoch [1/4], Step [2400/12942], Loss: 2.5239, Perplexity: 12.4766\n",
      "Epoch [1/4], Step [2500/12942], Loss: 2.2895, Perplexity: 9.87020\n",
      "Epoch [1/4], Step [2600/12942], Loss: 2.5886, Perplexity: 13.3117\n",
      "Epoch [1/4], Step [2700/12942], Loss: 2.2750, Perplexity: 9.72751\n",
      "Epoch [1/4], Step [2800/12942], Loss: 2.8210, Perplexity: 16.7944\n",
      "Epoch [1/4], Step [2900/12942], Loss: 2.5636, Perplexity: 12.9822\n",
      "Epoch [1/4], Step [3000/12942], Loss: 2.2264, Perplexity: 9.26665\n",
      "Epoch [1/4], Step [3100/12942], Loss: 2.5063, Perplexity: 12.2592\n",
      "Epoch [1/4], Step [3200/12942], Loss: 2.2494, Perplexity: 9.48249\n",
      "Epoch [1/4], Step [3300/12942], Loss: 2.5808, Perplexity: 13.2075\n",
      "Epoch [1/4], Step [3400/12942], Loss: 2.0671, Perplexity: 7.90221\n",
      "Epoch [1/4], Step [3500/12942], Loss: 2.4092, Perplexity: 11.1252\n",
      "Epoch [1/4], Step [3600/12942], Loss: 3.0407, Perplexity: 20.9204\n",
      "Epoch [1/4], Step [3700/12942], Loss: 2.6837, Perplexity: 14.6388\n",
      "Epoch [1/4], Step [3800/12942], Loss: 3.5165, Perplexity: 33.6668\n",
      "Epoch [1/4], Step [3900/12942], Loss: 2.2259, Perplexity: 9.26205\n",
      "Epoch [1/4], Step [4000/12942], Loss: 2.3432, Perplexity: 10.4144\n",
      "Epoch [1/4], Step [4100/12942], Loss: 2.2754, Perplexity: 9.73232\n",
      "Epoch [1/4], Step [4200/12942], Loss: 2.4260, Perplexity: 11.3130\n",
      "Epoch [1/4], Step [4300/12942], Loss: 2.2631, Perplexity: 9.61240\n",
      "Epoch [1/4], Step [4400/12942], Loss: 2.6373, Perplexity: 13.9759\n",
      "Epoch [1/4], Step [4500/12942], Loss: 2.3390, Perplexity: 10.3706\n",
      "Epoch [1/4], Step [4600/12942], Loss: 2.2378, Perplexity: 9.37256\n",
      "Epoch [1/4], Step [4700/12942], Loss: 2.2115, Perplexity: 9.12973\n",
      "Epoch [1/4], Step [4800/12942], Loss: 2.4282, Perplexity: 11.3380\n",
      "Epoch [1/4], Step [4900/12942], Loss: 2.2273, Perplexity: 9.27504\n",
      "Epoch [1/4], Step [5000/12942], Loss: 2.7995, Perplexity: 16.4362\n",
      "Epoch [1/4], Step [5100/12942], Loss: 2.5122, Perplexity: 12.3314\n",
      "Epoch [1/4], Step [5200/12942], Loss: 2.7847, Perplexity: 16.1950\n",
      "Epoch [1/4], Step [5300/12942], Loss: 2.3292, Perplexity: 10.2702\n",
      "Epoch [1/4], Step [5400/12942], Loss: 2.3676, Perplexity: 10.6718\n",
      "Epoch [1/4], Step [5500/12942], Loss: 2.1796, Perplexity: 8.84251\n",
      "Epoch [1/4], Step [5600/12942], Loss: 2.3929, Perplexity: 10.9453\n",
      "Epoch [1/4], Step [5700/12942], Loss: 2.2458, Perplexity: 9.44766\n",
      "Epoch [1/4], Step [5800/12942], Loss: 2.5748, Perplexity: 13.1284\n",
      "Epoch [1/4], Step [5900/12942], Loss: 2.5636, Perplexity: 12.9831\n",
      "Epoch [1/4], Step [6000/12942], Loss: 2.4047, Perplexity: 11.0749\n",
      "Epoch [1/4], Step [6100/12942], Loss: 2.4105, Perplexity: 11.1398\n",
      "Epoch [1/4], Step [6200/12942], Loss: 2.4234, Perplexity: 11.2846\n",
      "Epoch [1/4], Step [6300/12942], Loss: 2.7214, Perplexity: 15.2019\n",
      "Epoch [1/4], Step [6400/12942], Loss: 2.2222, Perplexity: 9.22723\n",
      "Epoch [1/4], Step [6500/12942], Loss: 1.9939, Perplexity: 7.34435\n",
      "Epoch [1/4], Step [6600/12942], Loss: 2.5348, Perplexity: 12.6137\n",
      "Epoch [1/4], Step [6700/12942], Loss: 2.8466, Perplexity: 17.2297\n",
      "Epoch [1/4], Step [6800/12942], Loss: 2.0161, Perplexity: 7.50922\n",
      "Epoch [1/4], Step [6900/12942], Loss: 2.3320, Perplexity: 10.2985\n",
      "Epoch [1/4], Step [7000/12942], Loss: 2.2721, Perplexity: 9.70000\n",
      "Epoch [1/4], Step [7100/12942], Loss: 2.3854, Perplexity: 10.8632\n",
      "Epoch [1/4], Step [7200/12942], Loss: 2.3353, Perplexity: 10.3327\n",
      "Epoch [1/4], Step [7300/12942], Loss: 2.0804, Perplexity: 8.00753\n",
      "Epoch [1/4], Step [7400/12942], Loss: 2.2349, Perplexity: 9.34586\n",
      "Epoch [1/4], Step [7500/12942], Loss: 2.5986, Perplexity: 13.4449\n",
      "Epoch [1/4], Step [7600/12942], Loss: 2.4534, Perplexity: 11.6275\n",
      "Epoch [1/4], Step [7700/12942], Loss: 2.3179, Perplexity: 10.1542\n",
      "Epoch [1/4], Step [7800/12942], Loss: 2.2509, Perplexity: 9.49670\n",
      "Epoch [1/4], Step [7900/12942], Loss: 2.2574, Perplexity: 9.55800\n",
      "Epoch [1/4], Step [8000/12942], Loss: 2.1674, Perplexity: 8.73558\n",
      "Epoch [1/4], Step [8100/12942], Loss: 2.2364, Perplexity: 9.35921\n",
      "Epoch [1/4], Step [8200/12942], Loss: 2.2842, Perplexity: 9.81743\n",
      "Epoch [1/4], Step [8300/12942], Loss: 2.0830, Perplexity: 8.02842\n",
      "Epoch [1/4], Step [8400/12942], Loss: 2.4045, Perplexity: 11.0733\n",
      "Epoch [1/4], Step [8500/12942], Loss: 2.2610, Perplexity: 9.59247\n",
      "Epoch [1/4], Step [8600/12942], Loss: 2.5622, Perplexity: 12.9645\n",
      "Epoch [1/4], Step [8700/12942], Loss: 2.3939, Perplexity: 10.9558\n",
      "Epoch [1/4], Step [8800/12942], Loss: 2.0991, Perplexity: 8.158851\n",
      "Epoch [1/4], Step [8900/12942], Loss: 2.0631, Perplexity: 7.87033\n",
      "Epoch [1/4], Step [9000/12942], Loss: 2.2858, Perplexity: 9.83339\n",
      "Epoch [1/4], Step [9100/12942], Loss: 2.5978, Perplexity: 13.4347\n",
      "Epoch [1/4], Step [9200/12942], Loss: 2.1023, Perplexity: 8.18473\n",
      "Epoch [1/4], Step [9300/12942], Loss: 2.1952, Perplexity: 8.98157\n",
      "Epoch [1/4], Step [9400/12942], Loss: 1.9428, Perplexity: 6.97805\n",
      "Epoch [1/4], Step [9500/12942], Loss: 2.3975, Perplexity: 10.9953\n",
      "Epoch [1/4], Step [9600/12942], Loss: 2.5576, Perplexity: 12.9047\n",
      "Epoch [1/4], Step [9700/12942], Loss: 2.1865, Perplexity: 8.90399\n",
      "Epoch [1/4], Step [9800/12942], Loss: 3.3735, Perplexity: 29.1812\n",
      "Epoch [1/4], Step [9900/12942], Loss: 2.3790, Perplexity: 10.7945\n",
      "Epoch [1/4], Step [10000/12942], Loss: 2.4410, Perplexity: 11.4848\n",
      "Epoch [1/4], Step [10100/12942], Loss: 1.8236, Perplexity: 6.19449\n",
      "Epoch [1/4], Step [10200/12942], Loss: 2.3824, Perplexity: 10.8310\n",
      "Epoch [1/4], Step [10300/12942], Loss: 2.2740, Perplexity: 9.71808\n",
      "Epoch [1/4], Step [10400/12942], Loss: 2.2067, Perplexity: 9.08551\n",
      "Epoch [1/4], Step [10500/12942], Loss: 2.1659, Perplexity: 8.72203\n",
      "Epoch [1/4], Step [10600/12942], Loss: 2.2754, Perplexity: 9.73151\n",
      "Epoch [1/4], Step [10700/12942], Loss: 2.5856, Perplexity: 13.2709\n",
      "Epoch [1/4], Step [10800/12942], Loss: 2.3472, Perplexity: 10.4567\n",
      "Epoch [1/4], Step [10900/12942], Loss: 2.0679, Perplexity: 7.90825\n",
      "Epoch [1/4], Step [11000/12942], Loss: 2.3874, Perplexity: 10.8857\n",
      "Epoch [1/4], Step [11100/12942], Loss: 2.0647, Perplexity: 7.88260\n",
      "Epoch [1/4], Step [11200/12942], Loss: 2.2562, Perplexity: 9.54679\n",
      "Epoch [1/4], Step [11300/12942], Loss: 1.9878, Perplexity: 7.29929\n",
      "Epoch [1/4], Step [11400/12942], Loss: 2.1525, Perplexity: 8.60646\n",
      "Epoch [1/4], Step [11500/12942], Loss: 2.6028, Perplexity: 13.5019\n",
      "Epoch [1/4], Step [11600/12942], Loss: 2.3943, Perplexity: 10.9609\n",
      "Epoch [1/4], Step [11700/12942], Loss: 2.2729, Perplexity: 9.70733\n",
      "Epoch [1/4], Step [11800/12942], Loss: 2.1885, Perplexity: 8.92179\n",
      "Epoch [1/4], Step [11900/12942], Loss: 2.2310, Perplexity: 9.30945\n",
      "Epoch [1/4], Step [12000/12942], Loss: 2.4945, Perplexity: 12.1156\n",
      "Epoch [1/4], Step [12100/12942], Loss: 2.0180, Perplexity: 7.52362\n",
      "Epoch [1/4], Step [12200/12942], Loss: 2.2184, Perplexity: 9.19297\n",
      "Epoch [1/4], Step [12300/12942], Loss: 2.9675, Perplexity: 19.4441\n",
      "Epoch [1/4], Step [12400/12942], Loss: 2.3180, Perplexity: 10.1558\n",
      "Epoch [1/4], Step [12500/12942], Loss: 2.1551, Perplexity: 8.62867\n",
      "Epoch [1/4], Step [12600/12942], Loss: 1.8655, Perplexity: 6.45900\n",
      "Epoch [1/4], Step [12700/12942], Loss: 2.3280, Perplexity: 10.2579\n",
      "Epoch [1/4], Step [12800/12942], Loss: 2.1391, Perplexity: 8.491627\n",
      "Epoch [1/4], Step [12900/12942], Loss: 2.2641, Perplexity: 9.62258\n",
      "Epoch [2/4], Step [100/12942], Loss: 2.1457, Perplexity: 8.5479436\n",
      "Epoch [2/4], Step [200/12942], Loss: 2.1848, Perplexity: 8.88853\n",
      "Epoch [2/4], Step [300/12942], Loss: 2.0911, Perplexity: 8.09358\n",
      "Epoch [2/4], Step [400/12942], Loss: 1.9948, Perplexity: 7.35107\n",
      "Epoch [2/4], Step [500/12942], Loss: 2.1373, Perplexity: 8.47683\n",
      "Epoch [2/4], Step [600/12942], Loss: 2.0910, Perplexity: 8.09292\n",
      "Epoch [2/4], Step [700/12942], Loss: 2.4844, Perplexity: 11.9944\n",
      "Epoch [2/4], Step [800/12942], Loss: 2.2287, Perplexity: 9.28821\n",
      "Epoch [2/4], Step [900/12942], Loss: 2.1079, Perplexity: 8.23076\n",
      "Epoch [2/4], Step [1000/12942], Loss: 2.8011, Perplexity: 16.4622\n",
      "Epoch [2/4], Step [1100/12942], Loss: 2.1854, Perplexity: 8.89401\n",
      "Epoch [2/4], Step [1200/12942], Loss: 1.9799, Perplexity: 7.24226\n",
      "Epoch [2/4], Step [1300/12942], Loss: 2.0087, Perplexity: 7.45385\n",
      "Epoch [2/4], Step [1400/12942], Loss: 2.2223, Perplexity: 9.22880\n",
      "Epoch [2/4], Step [1500/12942], Loss: 2.1155, Perplexity: 8.29386\n",
      "Epoch [2/4], Step [1600/12942], Loss: 2.6810, Perplexity: 14.6000\n",
      "Epoch [2/4], Step [1700/12942], Loss: 2.1753, Perplexity: 8.80497\n",
      "Epoch [2/4], Step [1800/12942], Loss: 2.0061, Perplexity: 7.43457\n",
      "Epoch [2/4], Step [1900/12942], Loss: 2.1264, Perplexity: 8.38429\n",
      "Epoch [2/4], Step [2000/12942], Loss: 1.8972, Perplexity: 6.66729\n",
      "Epoch [2/4], Step [2100/12942], Loss: 2.3937, Perplexity: 10.9540\n",
      "Epoch [2/4], Step [2200/12942], Loss: 2.0767, Perplexity: 7.97782\n",
      "Epoch [2/4], Step [2300/12942], Loss: 2.0983, Perplexity: 8.15230\n",
      "Epoch [2/4], Step [2400/12942], Loss: 1.9423, Perplexity: 6.97510\n",
      "Epoch [2/4], Step [2500/12942], Loss: 2.0377, Perplexity: 7.67260\n",
      "Epoch [2/4], Step [2600/12942], Loss: 2.2708, Perplexity: 9.68683\n",
      "Epoch [2/4], Step [2700/12942], Loss: 2.3258, Perplexity: 10.2349\n",
      "Epoch [2/4], Step [2800/12942], Loss: 2.2259, Perplexity: 9.26226\n",
      "Epoch [2/4], Step [2900/12942], Loss: 2.3115, Perplexity: 10.0894\n",
      "Epoch [2/4], Step [3000/12942], Loss: 2.1603, Perplexity: 8.67403\n",
      "Epoch [2/4], Step [3100/12942], Loss: 2.2151, Perplexity: 9.16236\n",
      "Epoch [2/4], Step [3200/12942], Loss: 2.4129, Perplexity: 11.1663\n",
      "Epoch [2/4], Step [3300/12942], Loss: 1.9765, Perplexity: 7.21738\n",
      "Epoch [2/4], Step [3400/12942], Loss: 2.3291, Perplexity: 10.2687\n",
      "Epoch [2/4], Step [3500/12942], Loss: 2.3126, Perplexity: 10.1002\n",
      "Epoch [2/4], Step [3600/12942], Loss: 2.2454, Perplexity: 9.44407\n",
      "Epoch [2/4], Step [3700/12942], Loss: 2.2024, Perplexity: 9.04634\n",
      "Epoch [2/4], Step [3800/12942], Loss: 2.4960, Perplexity: 12.1333\n",
      "Epoch [2/4], Step [3900/12942], Loss: 2.2261, Perplexity: 9.26408\n",
      "Epoch [2/4], Step [4000/12942], Loss: 2.4045, Perplexity: 11.0725\n",
      "Epoch [2/4], Step [4100/12942], Loss: 2.8799, Perplexity: 17.8120\n",
      "Epoch [2/4], Step [4200/12942], Loss: 2.0895, Perplexity: 8.08085\n",
      "Epoch [2/4], Step [4300/12942], Loss: 2.0157, Perplexity: 7.50599\n",
      "Epoch [2/4], Step [4400/12942], Loss: 1.9074, Perplexity: 6.73554\n",
      "Epoch [2/4], Step [4500/12942], Loss: 2.3968, Perplexity: 10.9882\n",
      "Epoch [2/4], Step [4600/12942], Loss: 2.6033, Perplexity: 13.5080\n",
      "Epoch [2/4], Step [4700/12942], Loss: 2.0768, Perplexity: 7.97899\n",
      "Epoch [2/4], Step [4800/12942], Loss: 2.4805, Perplexity: 11.9477\n",
      "Epoch [2/4], Step [4900/12942], Loss: 2.2103, Perplexity: 9.11845\n",
      "Epoch [2/4], Step [5000/12942], Loss: 2.0738, Perplexity: 7.95514\n",
      "Epoch [2/4], Step [5100/12942], Loss: 1.8471, Perplexity: 6.34116\n",
      "Epoch [2/4], Step [5200/12942], Loss: 2.0794, Perplexity: 7.99933\n",
      "Epoch [2/4], Step [5300/12942], Loss: 2.1440, Perplexity: 8.53355\n",
      "Epoch [2/4], Step [5400/12942], Loss: 1.9815, Perplexity: 7.25391\n",
      "Epoch [2/4], Step [5500/12942], Loss: 2.1228, Perplexity: 8.35442\n",
      "Epoch [2/4], Step [5600/12942], Loss: 2.8585, Perplexity: 17.4349\n",
      "Epoch [2/4], Step [5700/12942], Loss: 2.1847, Perplexity: 8.88806\n",
      "Epoch [2/4], Step [5800/12942], Loss: 2.0150, Perplexity: 7.50097\n",
      "Epoch [2/4], Step [5900/12942], Loss: 2.2011, Perplexity: 9.03460\n",
      "Epoch [2/4], Step [6000/12942], Loss: 2.0696, Perplexity: 7.92198\n",
      "Epoch [2/4], Step [6100/12942], Loss: 2.0845, Perplexity: 8.04065\n",
      "Epoch [2/4], Step [6200/12942], Loss: 2.3834, Perplexity: 10.8420\n",
      "Epoch [2/4], Step [6300/12942], Loss: 2.5246, Perplexity: 12.4863\n",
      "Epoch [2/4], Step [6400/12942], Loss: 2.1722, Perplexity: 8.77772\n",
      "Epoch [2/4], Step [6500/12942], Loss: 2.0994, Perplexity: 8.16138\n",
      "Epoch [2/4], Step [6600/12942], Loss: 2.3083, Perplexity: 10.0573\n",
      "Epoch [2/4], Step [6700/12942], Loss: 1.9464, Perplexity: 7.00384\n",
      "Epoch [2/4], Step [6800/12942], Loss: 1.8772, Perplexity: 6.53502\n",
      "Epoch [2/4], Step [6900/12942], Loss: 2.4419, Perplexity: 11.49534\n",
      "Epoch [2/4], Step [7000/12942], Loss: 2.0754, Perplexity: 7.96755\n",
      "Epoch [2/4], Step [7100/12942], Loss: 2.1346, Perplexity: 8.45365\n",
      "Epoch [2/4], Step [7200/12942], Loss: 2.1211, Perplexity: 8.34076\n",
      "Epoch [2/4], Step [7300/12942], Loss: 2.2132, Perplexity: 9.14494\n",
      "Epoch [2/4], Step [7400/12942], Loss: 2.2843, Perplexity: 9.81896\n",
      "Epoch [2/4], Step [7500/12942], Loss: 3.0165, Perplexity: 20.4200\n",
      "Epoch [2/4], Step [7600/12942], Loss: 2.0530, Perplexity: 7.79154\n",
      "Epoch [2/4], Step [7700/12942], Loss: 1.9272, Perplexity: 6.87012\n",
      "Epoch [2/4], Step [7800/12942], Loss: 2.0804, Perplexity: 8.00771\n",
      "Epoch [2/4], Step [7900/12942], Loss: 2.0647, Perplexity: 7.88322\n",
      "Epoch [2/4], Step [8000/12942], Loss: 1.9848, Perplexity: 7.27734\n",
      "Epoch [2/4], Step [8100/12942], Loss: 2.1004, Perplexity: 8.16971\n",
      "Epoch [2/4], Step [8200/12942], Loss: 2.4216, Perplexity: 11.2638\n",
      "Epoch [2/4], Step [8300/12942], Loss: 2.0393, Perplexity: 7.68517\n",
      "Epoch [2/4], Step [8400/12942], Loss: 2.1326, Perplexity: 8.43729\n",
      "Epoch [2/4], Step [8500/12942], Loss: 2.1386, Perplexity: 8.48723\n",
      "Epoch [2/4], Step [8600/12942], Loss: 1.8788, Perplexity: 6.54557\n",
      "Epoch [2/4], Step [8700/12942], Loss: 2.1076, Perplexity: 8.22856\n",
      "Epoch [2/4], Step [8800/12942], Loss: 1.9071, Perplexity: 6.73348\n",
      "Epoch [2/4], Step [8900/12942], Loss: 1.9592, Perplexity: 7.09338\n",
      "Epoch [2/4], Step [9000/12942], Loss: 2.0022, Perplexity: 7.40513\n",
      "Epoch [2/4], Step [9100/12942], Loss: 2.1725, Perplexity: 8.78030\n",
      "Epoch [2/4], Step [9200/12942], Loss: 2.1023, Perplexity: 8.18477\n",
      "Epoch [2/4], Step [9300/12942], Loss: 2.0200, Perplexity: 7.53862\n",
      "Epoch [2/4], Step [9400/12942], Loss: 1.9305, Perplexity: 6.89331\n",
      "Epoch [2/4], Step [9500/12942], Loss: 2.3973, Perplexity: 10.9929\n",
      "Epoch [2/4], Step [9600/12942], Loss: 1.8868, Perplexity: 6.59824\n",
      "Epoch [2/4], Step [9700/12942], Loss: 2.1103, Perplexity: 8.25087\n",
      "Epoch [2/4], Step [9800/12942], Loss: 1.9748, Perplexity: 7.20494\n",
      "Epoch [2/4], Step [9900/12942], Loss: 1.7943, Perplexity: 6.01544\n",
      "Epoch [2/4], Step [10000/12942], Loss: 2.2264, Perplexity: 9.2667\n",
      "Epoch [2/4], Step [10100/12942], Loss: 2.0266, Perplexity: 7.58828\n",
      "Epoch [2/4], Step [10200/12942], Loss: 1.9568, Perplexity: 7.07707\n",
      "Epoch [2/4], Step [10300/12942], Loss: 1.8497, Perplexity: 6.35799\n",
      "Epoch [2/4], Step [10400/12942], Loss: 1.8960, Perplexity: 6.65924\n",
      "Epoch [2/4], Step [10500/12942], Loss: 2.0256, Perplexity: 7.58050\n",
      "Epoch [2/4], Step [10600/12942], Loss: 2.0289, Perplexity: 7.60541\n",
      "Epoch [2/4], Step [10700/12942], Loss: 1.9133, Perplexity: 6.77565\n",
      "Epoch [2/4], Step [10800/12942], Loss: 1.9687, Perplexity: 7.16140\n",
      "Epoch [2/4], Step [10900/12942], Loss: 2.0074, Perplexity: 7.44384\n",
      "Epoch [2/4], Step [11000/12942], Loss: 1.9483, Perplexity: 7.01684\n",
      "Epoch [2/4], Step [11100/12942], Loss: 2.1468, Perplexity: 8.55713\n",
      "Epoch [2/4], Step [11200/12942], Loss: 2.0662, Perplexity: 7.89466\n",
      "Epoch [2/4], Step [11300/12942], Loss: 2.0375, Perplexity: 7.67129\n",
      "Epoch [2/4], Step [11400/12942], Loss: 1.9119, Perplexity: 6.76620\n",
      "Epoch [2/4], Step [11500/12942], Loss: 2.2111, Perplexity: 9.12591\n",
      "Epoch [2/4], Step [11600/12942], Loss: 1.9282, Perplexity: 6.87756\n",
      "Epoch [2/4], Step [11700/12942], Loss: 2.2412, Perplexity: 9.40472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/4], Step [11800/12942], Loss: 2.0179, Perplexity: 7.52212\n",
      "Epoch [2/4], Step [11900/12942], Loss: 2.2297, Perplexity: 9.29732\n",
      "Epoch [2/4], Step [12000/12942], Loss: 2.0726, Perplexity: 7.94573\n",
      "Epoch [2/4], Step [12100/12942], Loss: 2.0699, Perplexity: 7.92382\n",
      "Epoch [2/4], Step [12200/12942], Loss: 2.0083, Perplexity: 7.45106\n",
      "Epoch [2/4], Step [12300/12942], Loss: 1.9439, Perplexity: 6.98594\n",
      "Epoch [2/4], Step [12400/12942], Loss: 2.0087, Perplexity: 7.45340\n",
      "Epoch [2/4], Step [12500/12942], Loss: 2.3475, Perplexity: 10.4589\n",
      "Epoch [2/4], Step [12600/12942], Loss: 1.7707, Perplexity: 5.87493\n",
      "Epoch [2/4], Step [12700/12942], Loss: 2.2304, Perplexity: 9.30330\n",
      "Epoch [2/4], Step [12800/12942], Loss: 1.9398, Perplexity: 6.95756\n",
      "Epoch [2/4], Step [12900/12942], Loss: 2.1602, Perplexity: 8.67268\n",
      "Epoch [3/4], Step [100/12942], Loss: 2.2078, Perplexity: 9.0958534\n",
      "Epoch [3/4], Step [200/12942], Loss: 1.9982, Perplexity: 7.37594\n",
      "Epoch [3/4], Step [300/12942], Loss: 2.3524, Perplexity: 10.5111\n",
      "Epoch [3/4], Step [400/12942], Loss: 2.0415, Perplexity: 7.70183\n",
      "Epoch [3/4], Step [500/12942], Loss: 2.2530, Perplexity: 9.51649\n",
      "Epoch [3/4], Step [600/12942], Loss: 2.0790, Perplexity: 7.99692\n",
      "Epoch [3/4], Step [700/12942], Loss: 2.0745, Perplexity: 7.96094\n",
      "Epoch [3/4], Step [800/12942], Loss: 1.7997, Perplexity: 6.04815\n",
      "Epoch [3/4], Step [900/12942], Loss: 1.9798, Perplexity: 7.24140\n",
      "Epoch [3/4], Step [1000/12942], Loss: 2.0131, Perplexity: 7.4863\n",
      "Epoch [3/4], Step [1100/12942], Loss: 2.2867, Perplexity: 9.84228\n",
      "Epoch [3/4], Step [1200/12942], Loss: 2.0610, Perplexity: 7.85424\n",
      "Epoch [3/4], Step [1300/12942], Loss: 1.9400, Perplexity: 6.95895\n",
      "Epoch [3/4], Step [1400/12942], Loss: 2.1863, Perplexity: 8.90216\n",
      "Epoch [3/4], Step [1500/12942], Loss: 2.2831, Perplexity: 9.80721\n",
      "Epoch [3/4], Step [1600/12942], Loss: 1.8253, Perplexity: 6.20475\n",
      "Epoch [3/4], Step [1700/12942], Loss: 1.9011, Perplexity: 6.693286\n",
      "Epoch [3/4], Step [1800/12942], Loss: 2.1142, Perplexity: 8.28278\n",
      "Epoch [3/4], Step [1900/12942], Loss: 2.2822, Perplexity: 9.798532\n",
      "Epoch [3/4], Step [2000/12942], Loss: 1.8958, Perplexity: 6.65770\n",
      "Epoch [3/4], Step [2100/12942], Loss: 2.1134, Perplexity: 8.27619\n",
      "Epoch [3/4], Step [2200/12942], Loss: 2.1528, Perplexity: 8.60875\n",
      "Epoch [3/4], Step [2300/12942], Loss: 2.4154, Perplexity: 11.1945\n",
      "Epoch [3/4], Step [2400/12942], Loss: 1.9022, Perplexity: 6.70090\n",
      "Epoch [3/4], Step [2500/12942], Loss: 2.1962, Perplexity: 8.99065\n",
      "Epoch [3/4], Step [2600/12942], Loss: 2.0772, Perplexity: 7.98197\n",
      "Epoch [3/4], Step [2700/12942], Loss: 2.3522, Perplexity: 10.5087\n",
      "Epoch [3/4], Step [2800/12942], Loss: 2.4675, Perplexity: 11.7933\n",
      "Epoch [3/4], Step [2900/12942], Loss: 2.1148, Perplexity: 8.28782\n",
      "Epoch [3/4], Step [3000/12942], Loss: 2.1610, Perplexity: 8.67994\n",
      "Epoch [3/4], Step [3100/12942], Loss: 2.2720, Perplexity: 9.69869\n",
      "Epoch [3/4], Step [3200/12942], Loss: 2.0956, Perplexity: 8.13002\n",
      "Epoch [3/4], Step [3300/12942], Loss: 1.9930, Perplexity: 7.33792\n",
      "Epoch [3/4], Step [3400/12942], Loss: 1.9672, Perplexity: 7.15071\n",
      "Epoch [3/4], Step [3500/12942], Loss: 2.0309, Perplexity: 7.62107\n",
      "Epoch [3/4], Step [3600/12942], Loss: 2.2252, Perplexity: 9.25503\n",
      "Epoch [3/4], Step [3700/12942], Loss: 1.9810, Perplexity: 7.25000\n",
      "Epoch [3/4], Step [3800/12942], Loss: 2.0912, Perplexity: 8.09503\n",
      "Epoch [3/4], Step [3900/12942], Loss: 2.1052, Perplexity: 8.20915\n",
      "Epoch [3/4], Step [4000/12942], Loss: 2.1211, Perplexity: 8.34062\n",
      "Epoch [3/4], Step [4100/12942], Loss: 2.1009, Perplexity: 8.17331\n",
      "Epoch [3/4], Step [4200/12942], Loss: 2.2872, Perplexity: 9.84746\n",
      "Epoch [3/4], Step [4300/12942], Loss: 1.7348, Perplexity: 5.66771\n",
      "Epoch [3/4], Step [4400/12942], Loss: 2.0569, Perplexity: 7.82131\n",
      "Epoch [3/4], Step [4500/12942], Loss: 2.0814, Perplexity: 8.01588\n",
      "Epoch [3/4], Step [4600/12942], Loss: 2.0771, Perplexity: 7.98100\n",
      "Epoch [3/4], Step [4700/12942], Loss: 1.9059, Perplexity: 6.72540\n",
      "Epoch [3/4], Step [4800/12942], Loss: 1.7953, Perplexity: 6.02105\n",
      "Epoch [3/4], Step [4900/12942], Loss: 1.9158, Perplexity: 6.79266\n",
      "Epoch [3/4], Step [5000/12942], Loss: 1.9170, Perplexity: 6.80049\n",
      "Epoch [3/4], Step [5100/12942], Loss: 1.7997, Perplexity: 6.04783\n",
      "Epoch [3/4], Step [5200/12942], Loss: 1.9336, Perplexity: 6.91468\n",
      "Epoch [3/4], Step [5300/12942], Loss: 2.1658, Perplexity: 8.72154\n",
      "Epoch [3/4], Step [5400/12942], Loss: 2.2775, Perplexity: 9.75190\n",
      "Epoch [3/4], Step [5500/12942], Loss: 2.0980, Perplexity: 8.14985\n",
      "Epoch [3/4], Step [5600/12942], Loss: 1.9366, Perplexity: 6.93537\n",
      "Epoch [3/4], Step [5700/12942], Loss: 1.9388, Perplexity: 6.95064\n",
      "Epoch [3/4], Step [5800/12942], Loss: 2.0802, Perplexity: 8.00574\n",
      "Epoch [3/4], Step [5900/12942], Loss: 1.9350, Perplexity: 6.92409\n",
      "Epoch [3/4], Step [6000/12942], Loss: 2.1784, Perplexity: 8.83240\n",
      "Epoch [3/4], Step [6100/12942], Loss: 2.0697, Perplexity: 7.92286\n",
      "Epoch [3/4], Step [6200/12942], Loss: 2.1836, Perplexity: 8.87836\n",
      "Epoch [3/4], Step [6300/12942], Loss: 2.0914, Perplexity: 8.09595\n",
      "Epoch [3/4], Step [6400/12942], Loss: 1.8797, Perplexity: 6.55150\n",
      "Epoch [3/4], Step [6500/12942], Loss: 2.3147, Perplexity: 10.1219\n",
      "Epoch [3/4], Step [6600/12942], Loss: 2.0840, Perplexity: 8.03677\n",
      "Epoch [3/4], Step [6700/12942], Loss: 2.0351, Perplexity: 7.65305\n",
      "Epoch [3/4], Step [6800/12942], Loss: 2.2615, Perplexity: 9.59780\n",
      "Epoch [3/4], Step [6900/12942], Loss: 1.8793, Perplexity: 6.54893\n",
      "Epoch [3/4], Step [7000/12942], Loss: 1.9300, Perplexity: 6.88931\n",
      "Epoch [3/4], Step [7100/12942], Loss: 1.9367, Perplexity: 6.93567\n",
      "Epoch [3/4], Step [7200/12942], Loss: 2.1958, Perplexity: 8.98680\n",
      "Epoch [3/4], Step [7300/12942], Loss: 2.0907, Perplexity: 8.09096\n",
      "Epoch [3/4], Step [7400/12942], Loss: 2.0027, Perplexity: 7.40887\n",
      "Epoch [3/4], Step [7500/12942], Loss: 1.9833, Perplexity: 7.26706\n",
      "Epoch [3/4], Step [7600/12942], Loss: 2.1809, Perplexity: 8.85468\n",
      "Epoch [3/4], Step [7700/12942], Loss: 2.1708, Perplexity: 8.76541\n",
      "Epoch [3/4], Step [7800/12942], Loss: 2.6748, Perplexity: 14.5094\n",
      "Epoch [3/4], Step [7900/12942], Loss: 2.2362, Perplexity: 9.35801\n",
      "Epoch [3/4], Step [8000/12942], Loss: 1.8682, Perplexity: 6.47698\n",
      "Epoch [3/4], Step [8100/12942], Loss: 1.9510, Perplexity: 7.03608\n",
      "Epoch [3/4], Step [8200/12942], Loss: 2.0644, Perplexity: 7.88056\n",
      "Epoch [3/4], Step [8300/12942], Loss: 2.2133, Perplexity: 9.14566\n",
      "Epoch [3/4], Step [8400/12942], Loss: 2.0050, Perplexity: 7.42618\n",
      "Epoch [3/4], Step [8500/12942], Loss: 2.1316, Perplexity: 8.42809\n",
      "Epoch [3/4], Step [8600/12942], Loss: 1.8853, Perplexity: 6.58824\n",
      "Epoch [3/4], Step [8700/12942], Loss: 2.0942, Perplexity: 8.11913\n",
      "Epoch [3/4], Step [8800/12942], Loss: 2.4376, Perplexity: 11.4452\n",
      "Epoch [3/4], Step [8900/12942], Loss: 2.1407, Perplexity: 8.50513\n",
      "Epoch [3/4], Step [9000/12942], Loss: 1.8505, Perplexity: 6.36319\n",
      "Epoch [3/4], Step [9100/12942], Loss: 1.9179, Perplexity: 6.80664\n",
      "Epoch [3/4], Step [9200/12942], Loss: 2.4376, Perplexity: 11.4458\n",
      "Epoch [3/4], Step [9300/12942], Loss: 1.9519, Perplexity: 7.04215\n",
      "Epoch [3/4], Step [9400/12942], Loss: 2.0387, Perplexity: 7.68047\n",
      "Epoch [3/4], Step [9500/12942], Loss: 2.0396, Perplexity: 7.68778\n",
      "Epoch [3/4], Step [9600/12942], Loss: 2.1182, Perplexity: 8.31626\n",
      "Epoch [3/4], Step [9700/12942], Loss: 2.0203, Perplexity: 7.54087\n",
      "Epoch [3/4], Step [9800/12942], Loss: 1.9726, Perplexity: 7.18910\n",
      "Epoch [3/4], Step [9900/12942], Loss: 1.7437, Perplexity: 5.71839\n",
      "Epoch [3/4], Step [10000/12942], Loss: 2.0346, Perplexity: 7.6490\n",
      "Epoch [3/4], Step [10100/12942], Loss: 2.0212, Perplexity: 7.54752\n",
      "Epoch [3/4], Step [10200/12942], Loss: 2.0410, Perplexity: 7.69800\n",
      "Epoch [3/4], Step [10300/12942], Loss: 2.4312, Perplexity: 11.3725\n",
      "Epoch [3/4], Step [10400/12942], Loss: 2.2004, Perplexity: 9.02861\n",
      "Epoch [3/4], Step [10500/12942], Loss: 2.0506, Perplexity: 7.77227\n",
      "Epoch [3/4], Step [10600/12942], Loss: 2.2077, Perplexity: 9.095240\n",
      "Epoch [3/4], Step [10700/12942], Loss: 1.8039, Perplexity: 6.07335\n",
      "Epoch [3/4], Step [10800/12942], Loss: 1.9906, Perplexity: 7.31989\n",
      "Epoch [3/4], Step [10900/12942], Loss: 2.0711, Perplexity: 7.93321\n",
      "Epoch [3/4], Step [11000/12942], Loss: 1.9451, Perplexity: 6.99449\n",
      "Epoch [3/4], Step [11100/12942], Loss: 1.7975, Perplexity: 6.03475\n",
      "Epoch [3/4], Step [11200/12942], Loss: 2.2031, Perplexity: 9.05293\n",
      "Epoch [3/4], Step [11300/12942], Loss: 1.7697, Perplexity: 5.86884\n",
      "Epoch [3/4], Step [11400/12942], Loss: 2.3673, Perplexity: 10.6689\n",
      "Epoch [3/4], Step [11500/12942], Loss: 2.0279, Perplexity: 7.59832\n",
      "Epoch [3/4], Step [11600/12942], Loss: 2.1574, Perplexity: 8.64842\n",
      "Epoch [3/4], Step [11700/12942], Loss: 1.9555, Perplexity: 7.06777\n",
      "Epoch [3/4], Step [11800/12942], Loss: 1.9118, Perplexity: 6.76507\n",
      "Epoch [3/4], Step [11900/12942], Loss: 2.2285, Perplexity: 9.28568\n",
      "Epoch [3/4], Step [12000/12942], Loss: 1.9002, Perplexity: 6.68710\n",
      "Epoch [3/4], Step [12100/12942], Loss: 2.1496, Perplexity: 8.58139\n",
      "Epoch [3/4], Step [12200/12942], Loss: 1.9803, Perplexity: 7.24494\n",
      "Epoch [3/4], Step [12300/12942], Loss: 2.0250, Perplexity: 7.57630\n",
      "Epoch [3/4], Step [12400/12942], Loss: 1.9438, Perplexity: 6.98539\n",
      "Epoch [3/4], Step [12500/12942], Loss: 2.2314, Perplexity: 9.31311\n",
      "Epoch [3/4], Step [12600/12942], Loss: 1.9069, Perplexity: 6.73200\n",
      "Epoch [3/4], Step [12700/12942], Loss: 2.1829, Perplexity: 8.87221\n",
      "Epoch [3/4], Step [12800/12942], Loss: 2.0520, Perplexity: 7.78378\n",
      "Epoch [3/4], Step [12900/12942], Loss: 1.9195, Perplexity: 6.81784\n",
      "Epoch [4/4], Step [100/12942], Loss: 2.2256, Perplexity: 9.2594801\n",
      "Epoch [4/4], Step [200/12942], Loss: 1.9900, Perplexity: 7.31560\n",
      "Epoch [4/4], Step [300/12942], Loss: 2.1532, Perplexity: 8.61275\n",
      "Epoch [4/4], Step [400/12942], Loss: 2.1562, Perplexity: 8.63793\n",
      "Epoch [4/4], Step [500/12942], Loss: 2.1578, Perplexity: 8.65240\n",
      "Epoch [4/4], Step [600/12942], Loss: 2.0426, Perplexity: 7.71064\n",
      "Epoch [4/4], Step [700/12942], Loss: 2.0575, Perplexity: 7.82610\n",
      "Epoch [4/4], Step [800/12942], Loss: 2.3109, Perplexity: 10.0839\n",
      "Epoch [4/4], Step [900/12942], Loss: 2.0300, Perplexity: 7.61429\n",
      "Epoch [4/4], Step [1000/12942], Loss: 2.0637, Perplexity: 7.8749\n",
      "Epoch [4/4], Step [1100/12942], Loss: 1.9966, Perplexity: 7.36428\n",
      "Epoch [4/4], Step [1200/12942], Loss: 1.8607, Perplexity: 6.42845\n",
      "Epoch [4/4], Step [1300/12942], Loss: 1.9796, Perplexity: 7.23961\n",
      "Epoch [4/4], Step [1400/12942], Loss: 2.0503, Perplexity: 7.77056\n",
      "Epoch [4/4], Step [1500/12942], Loss: 1.9137, Perplexity: 6.77797\n",
      "Epoch [4/4], Step [1600/12942], Loss: 2.0299, Perplexity: 7.61314\n",
      "Epoch [4/4], Step [1700/12942], Loss: 1.8829, Perplexity: 6.57276\n",
      "Epoch [4/4], Step [1800/12942], Loss: 1.8955, Perplexity: 6.65560\n",
      "Epoch [4/4], Step [1900/12942], Loss: 2.1000, Perplexity: 8.16583\n",
      "Epoch [4/4], Step [2000/12942], Loss: 1.9279, Perplexity: 6.87488\n",
      "Epoch [4/4], Step [2100/12942], Loss: 2.0437, Perplexity: 7.71940\n",
      "Epoch [4/4], Step [2200/12942], Loss: 1.9987, Perplexity: 7.37944\n",
      "Epoch [4/4], Step [2300/12942], Loss: 2.2163, Perplexity: 9.17323\n",
      "Epoch [4/4], Step [2400/12942], Loss: 2.2175, Perplexity: 9.18415\n",
      "Epoch [4/4], Step [2500/12942], Loss: 2.0751, Perplexity: 7.96578\n",
      "Epoch [4/4], Step [2600/12942], Loss: 1.9687, Perplexity: 7.16113\n",
      "Epoch [4/4], Step [2700/12942], Loss: 1.8980, Perplexity: 6.67291\n",
      "Epoch [4/4], Step [2800/12942], Loss: 1.7377, Perplexity: 5.68442\n",
      "Epoch [4/4], Step [2900/12942], Loss: 2.0658, Perplexity: 7.89162\n",
      "Epoch [4/4], Step [3000/12942], Loss: 2.2396, Perplexity: 9.38932\n",
      "Epoch [4/4], Step [3100/12942], Loss: 1.8646, Perplexity: 6.45354\n",
      "Epoch [4/4], Step [3200/12942], Loss: 2.0410, Perplexity: 7.69836\n",
      "Epoch [4/4], Step [3300/12942], Loss: 1.9995, Perplexity: 7.38505\n",
      "Epoch [4/4], Step [3400/12942], Loss: 1.7821, Perplexity: 5.94211\n",
      "Epoch [4/4], Step [3500/12942], Loss: 1.8094, Perplexity: 6.10696\n",
      "Epoch [4/4], Step [3600/12942], Loss: 2.0449, Perplexity: 7.72864\n",
      "Epoch [4/4], Step [3700/12942], Loss: 2.2286, Perplexity: 9.28697\n",
      "Epoch [4/4], Step [3800/12942], Loss: 2.1386, Perplexity: 8.48726\n",
      "Epoch [4/4], Step [3900/12942], Loss: 1.9041, Perplexity: 6.71353\n",
      "Epoch [4/4], Step [4000/12942], Loss: 1.9988, Perplexity: 7.38002\n",
      "Epoch [4/4], Step [4100/12942], Loss: 1.9881, Perplexity: 7.30138\n",
      "Epoch [4/4], Step [4200/12942], Loss: 1.7796, Perplexity: 5.92724\n",
      "Epoch [4/4], Step [4300/12942], Loss: 1.9684, Perplexity: 7.15908\n",
      "Epoch [4/4], Step [4400/12942], Loss: 2.0003, Perplexity: 7.39143\n",
      "Epoch [4/4], Step [4500/12942], Loss: 2.0639, Perplexity: 7.87681\n",
      "Epoch [4/4], Step [4600/12942], Loss: 1.8838, Perplexity: 6.57850\n",
      "Epoch [4/4], Step [4700/12942], Loss: 2.5617, Perplexity: 12.9575\n",
      "Epoch [4/4], Step [4800/12942], Loss: 2.0211, Perplexity: 7.54683\n",
      "Epoch [4/4], Step [4900/12942], Loss: 2.4592, Perplexity: 11.6959\n",
      "Epoch [4/4], Step [5000/12942], Loss: 1.9514, Perplexity: 7.03862\n",
      "Epoch [4/4], Step [5100/12942], Loss: 1.9939, Perplexity: 7.34449\n",
      "Epoch [4/4], Step [5200/12942], Loss: 1.9356, Perplexity: 6.92849\n",
      "Epoch [4/4], Step [5300/12942], Loss: 1.9254, Perplexity: 6.85770\n",
      "Epoch [4/4], Step [5400/12942], Loss: 2.1144, Perplexity: 8.28475\n",
      "Epoch [4/4], Step [5500/12942], Loss: 1.8845, Perplexity: 6.58306\n",
      "Epoch [4/4], Step [5600/12942], Loss: 2.0836, Perplexity: 8.03338\n",
      "Epoch [4/4], Step [5700/12942], Loss: 1.8411, Perplexity: 6.30387\n",
      "Epoch [4/4], Step [5800/12942], Loss: 2.2237, Perplexity: 9.24135\n",
      "Epoch [4/4], Step [5900/12942], Loss: 2.3890, Perplexity: 10.9029\n",
      "Epoch [4/4], Step [6000/12942], Loss: 1.8162, Perplexity: 6.14861\n",
      "Epoch [4/4], Step [6100/12942], Loss: 2.1544, Perplexity: 8.62275\n",
      "Epoch [4/4], Step [6200/12942], Loss: 1.7864, Perplexity: 5.96773\n",
      "Epoch [4/4], Step [6300/12942], Loss: 1.8295, Perplexity: 6.23067\n",
      "Epoch [4/4], Step [6400/12942], Loss: 2.1595, Perplexity: 8.66710\n",
      "Epoch [4/4], Step [6500/12942], Loss: 2.0907, Perplexity: 8.09095\n",
      "Epoch [4/4], Step [6600/12942], Loss: 2.1357, Perplexity: 8.46326\n",
      "Epoch [4/4], Step [6700/12942], Loss: 1.7397, Perplexity: 5.69547\n",
      "Epoch [4/4], Step [6800/12942], Loss: 2.0325, Perplexity: 7.63304\n",
      "Epoch [4/4], Step [6900/12942], Loss: 1.8464, Perplexity: 6.33723\n",
      "Epoch [4/4], Step [7000/12942], Loss: 1.9644, Perplexity: 7.13037\n",
      "Epoch [4/4], Step [7100/12942], Loss: 2.0047, Perplexity: 7.42419\n",
      "Epoch [4/4], Step [7200/12942], Loss: 1.9428, Perplexity: 6.97799\n",
      "Epoch [4/4], Step [7300/12942], Loss: 1.7403, Perplexity: 5.69915\n",
      "Epoch [4/4], Step [7400/12942], Loss: 1.9509, Perplexity: 7.034847\n",
      "Epoch [4/4], Step [7500/12942], Loss: 1.8532, Perplexity: 6.38050\n",
      "Epoch [4/4], Step [7600/12942], Loss: 2.0699, Perplexity: 7.92396\n",
      "Epoch [4/4], Step [7700/12942], Loss: 1.8972, Perplexity: 6.66758\n",
      "Epoch [4/4], Step [7800/12942], Loss: 2.5409, Perplexity: 12.6913\n",
      "Epoch [4/4], Step [7900/12942], Loss: 1.8962, Perplexity: 6.66079\n",
      "Epoch [4/4], Step [8000/12942], Loss: 1.9629, Perplexity: 7.11992\n",
      "Epoch [4/4], Step [8100/12942], Loss: 1.8066, Perplexity: 6.08991\n",
      "Epoch [4/4], Step [8200/12942], Loss: 2.0272, Perplexity: 7.59267\n",
      "Epoch [4/4], Step [8300/12942], Loss: 1.8544, Perplexity: 6.38791\n",
      "Epoch [4/4], Step [8400/12942], Loss: 1.7641, Perplexity: 5.83637\n",
      "Epoch [4/4], Step [8500/12942], Loss: 2.0392, Perplexity: 7.68449\n",
      "Epoch [4/4], Step [8600/12942], Loss: 1.8314, Perplexity: 6.24266\n",
      "Epoch [4/4], Step [8700/12942], Loss: 1.9648, Perplexity: 7.13339\n",
      "Epoch [4/4], Step [8800/12942], Loss: 1.9958, Perplexity: 7.35795\n",
      "Epoch [4/4], Step [8900/12942], Loss: 1.7092, Perplexity: 5.52471\n",
      "Epoch [4/4], Step [9000/12942], Loss: 1.7445, Perplexity: 5.72327\n",
      "Epoch [4/4], Step [9100/12942], Loss: 1.8611, Perplexity: 6.43064\n",
      "Epoch [4/4], Step [9200/12942], Loss: 1.8454, Perplexity: 6.33094\n",
      "Epoch [4/4], Step [9300/12942], Loss: 1.9155, Perplexity: 6.79072\n",
      "Epoch [4/4], Step [9400/12942], Loss: 2.1233, Perplexity: 8.35899\n",
      "Epoch [4/4], Step [9500/12942], Loss: 1.9546, Perplexity: 7.06101\n",
      "Epoch [4/4], Step [9600/12942], Loss: 2.0403, Perplexity: 7.69325\n",
      "Epoch [4/4], Step [9700/12942], Loss: 1.9220, Perplexity: 6.83476\n",
      "Epoch [4/4], Step [9800/12942], Loss: 2.2587, Perplexity: 9.57093\n",
      "Epoch [4/4], Step [9900/12942], Loss: 1.8188, Perplexity: 6.16427\n",
      "Epoch [4/4], Step [10000/12942], Loss: 2.0564, Perplexity: 7.8174\n",
      "Epoch [4/4], Step [10100/12942], Loss: 1.9158, Perplexity: 6.79265\n",
      "Epoch [4/4], Step [10200/12942], Loss: 1.9756, Perplexity: 7.21127\n",
      "Epoch [4/4], Step [10300/12942], Loss: 2.0786, Perplexity: 7.99346\n",
      "Epoch [4/4], Step [10400/12942], Loss: 2.1757, Perplexity: 8.80869\n",
      "Epoch [4/4], Step [10500/12942], Loss: 1.9923, Perplexity: 7.33244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/4], Step [10600/12942], Loss: 1.9802, Perplexity: 7.24390\n",
      "Epoch [4/4], Step [10700/12942], Loss: 1.9025, Perplexity: 6.70254\n",
      "Epoch [4/4], Step [10800/12942], Loss: 2.4929, Perplexity: 12.0967\n",
      "Epoch [4/4], Step [10868/12942], Loss: 2.0709, Perplexity: 7.93215"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
